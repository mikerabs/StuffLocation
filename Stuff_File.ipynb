{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing Basic libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model, naive_bayes\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "import bokeh as bk\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "from chart_studio.plotly import plot, iplot \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objects as go\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import pybaseball as pyb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from pybaseball import statcast\n",
    "from pybaseball import statcast_pitcher\n",
    "from pybaseball import statcast_batter\n",
    "from pybaseball import statcast_pitcher_exitvelo_barrels\n",
    "from pybaseball import statcast_batter_exitvelo_barrels\n",
    "from pybaseball import statcast_batter_expected_stats\n",
    "from pybaseball import statcast_pitcher_expected_stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pwlf as pwlf\n",
    "#import matplotlib.backends\n",
    "import matplotlib as mpl\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "#from test import keras\n",
    "from keras.models import Sequential\n",
    "#dense\n",
    "from keras.layers import Dense\n",
    "#adam\n",
    "from keras.optimizers import Adam\n",
    "from datetime import datetime, timedelta\n",
    "from pybaseball import statcast\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data, Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      "  7%|▋         | 1/15 [00:06<01:29,  6.42s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 13%|█▎        | 2/15 [00:15<01:42,  7.91s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 20%|██        | 3/15 [00:21<01:25,  7.16s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 27%|██▋       | 4/15 [00:21<00:48,  4.39s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 33%|███▎      | 5/15 [00:22<00:29,  2.95s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 40%|████      | 6/15 [00:22<00:18,  2.02s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 47%|████▋     | 7/15 [00:26<00:21,  2.63s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 53%|█████▎    | 8/15 [00:26<00:13,  1.92s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 60%|██████    | 9/15 [00:26<00:08,  1.37s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 67%|██████▋   | 10/15 [00:26<00:04,  1.01it/s]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 73%|███████▎  | 11/15 [00:27<00:03,  1.24it/s]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 80%|████████  | 12/15 [00:29<00:03,  1.07s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 87%|████████▋ | 13/15 [00:29<00:02,  1.02s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 93%|█████████▎| 14/15 [00:30<00:00,  1.32it/s]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      "100%|██████████| 15/15 [00:33<00:00,  2.20s/it]\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:29: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:30: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:84: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:86: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:87: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2022 = pd.read_csv(\"/Users/mikerabayda/Downloads/MLB_ALL_TIME.csv\",low_memory=False)\n",
    "df_2024 = pd.read_csv(\"/Users/mikerabayda/Downloads/Statcast_2024.csv\",low_memory=False)\n",
    "df_2022 = pd.concat([df_2022, df_2024], ignore_index=True)\n",
    "# Automatically use yesterday's date\n",
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "# Pull Statcast data for yesterday only\n",
    "df_2025 = statcast(start_dt='2025-03-27', end_dt=yesterday_str)\n",
    "df_2023 = df_2025 \n",
    "\n",
    "#first four digits of the date are the year game_date\n",
    "df_2022['Year'] = df_2022['game_date'].astype(str).str[:4]\n",
    "df_2023['Year'] = df_2023['game_date'].astype(str).str[:4]\n",
    "#remove 2023 data from 2022 data\n",
    "#multiply pfx_z\tpfx_x by 12 to get inches\n",
    "df_2022['pfx_x'] = df_2022['pfx_x']*12\n",
    "df_2022['pfx_z'] = df_2022['pfx_z']*12\n",
    "df_2023['pfx_x'] = df_2023['pfx_x']*12\n",
    "df_2023['pfx_z'] = df_2023['pfx_z']*12\n",
    "\n",
    "\n",
    "#multiply pfx_x by -1 to get pitchers view\n",
    "df_2022['pfx_x'] = df_2022['pfx_x']*-1\n",
    "df_2023['pfx_x'] = df_2023['pfx_x']*-1\n",
    "\n",
    "df_2022.rename(columns={'game_date': 'Date', 'player_name':'Pitcher','p_throws':'Hand','pitch_type':'TaggedPitchType','release_speed':'RelSpeed','release_pos_x':'RelSide','release_pos_z':'RelHeight','pfx_x':'HorzBreak','pfx_z':'InducedVertBreak','release_extension':'Extension','plate_x': 'PlateLocSide','plate_z': 'PlateLocHeight', 'release_spin_rate':'SpinRate'},inplace=True)\n",
    "df_2023.rename(columns={'game_date': 'Date', 'player_name':'Pitcher','pitcher':'Pitcher_ID','p_throws':'Hand','pitch_type':'TaggedPitchType','release_speed':'RelSpeed','release_pos_x':'RelSide','release_pos_z':'RelHeight','pfx_x':'HorzBreak','pfx_z':'InducedVertBreak','release_extension':'Extension','plate_x': 'PlateLocSide','plate_z': 'PlateLocHeight','release_spin_rate':'SpinRate'},inplace=True)\n",
    "#if Event is == NaN fill with description\n",
    "df_2022['events'].fillna(df_2022['description'], inplace=True)\n",
    "df_2023['events'].fillna(df_2023['description'], inplace=True)\n",
    "\n",
    "df_2023['PitcherTeam'] = np.where(df_2023['inning_topbot'] == 'Bot', df_2023['away_team'], df_2023['home_team'])\n",
    "#df_2023 Pitcher\tTaggedPitchType\tRelSpeed\trelease_spin_rate\tInducedVertBreak\tHorzBreak\tRelSide\tRelHeight\tExtension\t\n",
    "df_2022 = df_2022[['Year','Pitcher','Hand','TaggedPitchType','events','description','RelSpeed','SpinRate','InducedVertBreak','HorzBreak','RelSide','RelHeight','Extension','PlateLocSide','PlateLocHeight','batter','launch_speed','launch_angle','hit_distance_sc','delta_run_exp']]\n",
    "df_2023 = df_2023[['Year','Pitcher','Pitcher_ID','PitcherTeam','Hand','TaggedPitchType','events','description','RelSpeed','SpinRate','InducedVertBreak','HorzBreak','RelSide','RelHeight','Extension','PlateLocSide','PlateLocHeight','batter','launch_speed','launch_angle','hit_distance_sc','delta_run_exp','bat_speed']]\n",
    "#Pitcher/Pitcher_ID / \n",
    "#run values \n",
    "field_out = -0.1955687665555\n",
    "force_out = -0.1955687665555\n",
    "other_out = -0.1955687665555\n",
    "fielders_choice_out = -0.1955687665555\n",
    "called_strike = -0.118124935770601\n",
    "swinging_strike = -0.118124935770601\n",
    "ball = 0.0636883289483747\n",
    "foul = -0.0380502742575014\n",
    "single = 0.467292970729251\n",
    "double = 0.766083122898271\n",
    "triple = 1.05755624961515\n",
    "home_run = 1.374328827219\n",
    "strikeout = -0.118124935770601\n",
    "fielders_choice = -0.1955687665555\n",
    "hit_by_pitch = 0.0636883289483747\n",
    "walk = 0.0636883289483747\n",
    "field_error = -0.1955687665555\n",
    "walk = 0.0636883289483747\n",
    "sac_fly = 0.0636883289483747\n",
    "double_play = -0.1955687665555\n",
    "wild_pitch =0.0636883289483747\n",
    "blocked_ball = 0.0636883289483747\n",
    "grounded_into_double_play = -0.1955687665555\n",
    "foul_bunt = -0.0380502742575014\n",
    "foul_tip = -0.0380502742575014\n",
    "sac_bunt_double_play = -0.1955687665555\n",
    "swinging_strike_blocked = -0.118124935770601\n",
    "missed_bunt = -0.118124935770601\n",
    "sac_bunt = 0.0636883289483747\n",
    "pitchout = 0\n",
    "caught_stealing_2b = 0\n",
    "bunt_foul_tip = -0.0380502742575014\n",
    "strikeout_double_play = -0.118124935770601\n",
    "pickoff_3b = 0 \n",
    "catcher_interf = 0\n",
    "caught_stealing_3b = 0 \n",
    "pickoff_caught_stealing_2b = 0\n",
    "triple_play = -0.118124935770601\n",
    "caught_stealing_home = 0 \n",
    "sac_fly_double_play = -0.1955687665555\n",
    "pickoff_1b = 0 \n",
    "pickoff_caught_stealing_home = 0\n",
    "pickoff_caught_stealing_3b = 0 \n",
    "game_advisory = 0\n",
    "pickoff_2b = 0\n",
    "df_2022['RunValue'] = df_2022['events'].map({'field_out':field_out,'force_out':force_out,'other_out':other_out,'fielders_choice_out':fielders_choice_out,'called_strike':called_strike,'swinging_strike':swinging_strike,'ball':ball,'foul':foul,'single':single,'double':double,'triple':triple,'home_run':home_run,'strikeout':strikeout,'fielders_choice':fielders_choice,'hit_by_pitch':hit_by_pitch,'walk':walk,'field_error':field_error,'walk':walk,'sac_fly':sac_fly,'double_play':double_play,'wild_pitch':wild_pitch,'blocked_ball':blocked_ball,'grounded_into_double_play':grounded_into_double_play,'foul_bunt':foul_bunt,'foul_tip':foul_tip,'sac_bunt_double_play':sac_bunt_double_play,'swinging_strike_blocked':swinging_strike_blocked,'missed_bunt':missed_bunt,'sac_bunt':sac_bunt,'pitchout':pitchout,'caught_stealing_2b':caught_stealing_2b,'bunt_foul_tip':bunt_foul_tip,'strikeout_double_play':strikeout_double_play,'pickoff_3b':pickoff_3b,'catcher_interf':catcher_interf,'caught_stealing_3b':caught_stealing_3b,'pickoff_caught_stealing_2b':pickoff_caught_stealing_2b,'triple_play':triple_play,'caught_stealing_home':caught_stealing_home,'sac_fly_double_play':sac_fly_double_play,'pickoff_1b':pickoff_1b,'pickoff_caught_stealing_home':pickoff_caught_stealing_home,'pickoff_caught_stealing_3b':pickoff_caught_stealing_3b,'game_advisory':game_advisory,'pickoff_2b':pickoff_2b})\n",
    "df_2023['RunValue'] = df_2023['events'].map({'field_out':field_out,'force_out':force_out,'other_out':other_out,'fielders_choice_out':fielders_choice_out,'called_strike':called_strike,'swinging_strike':swinging_strike,'ball':ball,'foul':foul,'single':single,'double':double,'triple':triple,'home_run':home_run,'strikeout':strikeout,'fielders_choice':fielders_choice,'hit_by_pitch':hit_by_pitch,'walk':walk,'field_error':field_error,'walk':walk,'sac_fly':sac_fly,'double_play':double_play,'wild_pitch':wild_pitch,'blocked_ball':blocked_ball,'grounded_into_double_play':grounded_into_double_play,'foul_bunt':foul_bunt,'foul_tip':foul_tip,'sac_bunt_double_play':sac_bunt_double_play,'swinging_strike_blocked':swinging_strike_blocked,'missed_bunt':missed_bunt,'sac_bunt':sac_bunt,'pitchout':pitchout,'caught_stealing_2b':caught_stealing_2b,'bunt_foul_tip':bunt_foul_tip,'strikeout_double_play':strikeout_double_play,'pickoff_3b':pickoff_3b,'catcher_interf':catcher_interf,'caught_stealing_3b':caught_stealing_3b,'pickoff_caught_stealing_2b':pickoff_caught_stealing_2b,'triple_play':triple_play,'caught_stealing_home':caught_stealing_home,'sac_fly_double_play':sac_fly_double_play,'pickoff_1b':pickoff_1b,'pickoff_caught_stealing_home':pickoff_caught_stealing_home,'pickoff_caught_stealing_3b':pickoff_caught_stealing_3b,'game_advisory':game_advisory,'pickoff_2b':pickoff_2b})\n",
    "#replace all Nan Events with Descriptions\n",
    "df_2022['events'].fillna(df_2022['description'], inplace=True)\n",
    "df_2023['events'].fillna(df_2023['description'], inplace=True)\n",
    "#filter only balls in play == hit_into_play\n",
    "df_2022_hit_into_play = df_2022[df_2022['description'] == 'hit_into_play']\n",
    "df_2023_hit_into_play = df_2023[df_2023['description'] == 'hit_into_play']\n",
    "df_not_hit_into_play = df_2022[df_2022['description'] != 'hit_into_play']\n",
    "df_not_hit_into_play_2023 = df_2023[df_2023['description'] != 'hit_into_play']\n",
    "#predict run value based on launch_speed','launch_angle','hit_distanc_sc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predict Run Value columns based on Launch Speed, Launch Angle, Hit Distance into df_2022, df_2023 on pitches hit into play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[21:06:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xRun Value EV/LA Best  R^2: 0.99902006185565506158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#predict run value based on launch speed, launch angle, hit distance\n",
    "X = df_2022_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']]\n",
    "y = df_2022_hit_into_play['RunValue']\n",
    "X1 = df_2023_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']]\n",
    "y1 = df_2023_hit_into_play['RunValue']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X1_train, X1_test, y1_train, y1_test =  train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already split into X_train, y_train:\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X1_train, y1_train, test_size=0.2, random_state=42)\n",
    "import numpy as np\n",
    "\n",
    "# Check for NaN or infinity in y1_train\n",
    "print(np.isnan(y1_train).sum(), np.isinf(y1_train).sum())\n",
    "print(np.isnan(y_val_1).sum(), np.isinf(y_val_1).sum())\n",
    "y1_train_clean = y1_train[np.isfinite(y1_train)]\n",
    "y_val_1_clean = y_val_1[np.isfinite(y_val_1)]\n",
    "X1_train_clean = X1_train[np.isfinite(y1_train)]\n",
    "X_val_1_clean = X_val_1[np.isfinite(y_val_1)]\n",
    "print(np.isnan(y1_train_clean).sum(), np.isinf(y1_train_clean).sum())\n",
    "print(np.isnan(y_val_1_clean).sum(), np.isinf(y_val_1_clean).sum())\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(**params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "xgb_model_1 = XGBRegressor(**params)\n",
    "xgb_model_1.fit(X1_train_clean, y1_train_clean, eval_set=[(X_val_1_clean, y_val_1_clean)], verbose=False)\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "n_estimators = 10  # Number of XGBoost models in the ensemble\n",
    "predictions = []\n",
    "\n",
    "# 1. Create bootstrap samples and train XGBoost models\n",
    "for i in range(n_estimators):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train)\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train_sample, y_train_sample, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # 2. Predict on validation set\n",
    "    preds = model.predict(X_val)\n",
    "    predictions.append(preds)\n",
    "\n",
    "# 3. Average predictions from all models for ensemble prediction\n",
    "ensemble_preds = np.mean(predictions, axis=0)\n",
    "\n",
    "# Calculate R^2 for ensemble prediction\n",
    "ensemble_r2 = r2_score(y_val, ensemble_preds)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = xgb.cv(\n",
    "    dtrain=dtrain, \n",
    "    params=params, \n",
    "    nfold=5, \n",
    "    num_boost_round=200, \n",
    " \n",
    "    metrics=\"rmse\",\n",
    "    as_pandas=True, \n",
    "    seed=42\n",
    ")\n",
    "#Variance \n",
    "variance_y = y_train.var()\n",
    "best_r_squared = 1 - (cv_results['test-rmse-mean'].min() ** 2) / variance_y\n",
    "#print best r^2 without using scientific notation\n",
    "print('xRun Value EV/LA Best  R^2:', '{:.20f}'.format(best_r_squared))\n",
    "df_2022_hit_into_play['RunValue'] = xgb_model.predict(df_2022_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']])\n",
    "df_2023_hit_into_play['RunValue'] = xgb_model_1.predict(df_2023_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Concatenate dfs with Run Value with balls not hit into play\n",
    "# 4. Add new features(ABS_Horizontal, ABS_RelSide, differntial_break), Split dataframes into specific pitches(FB, Sink, Slider, Sweeper, Changeup, Cutter, Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#combine df_2022_hit_into_play and df_not_hit_into_play \n",
    "df_2022_1 = pd.concat([df_2022_hit_into_play,df_not_hit_into_play ],ignore_index=True)\n",
    "df_2023_1 = pd.concat([df_2023_hit_into_play,df_not_hit_into_play_2023 ],ignore_index=True)\n",
    "df_2022 = df_2022_1\n",
    "df_2023 = df_2023_1\n",
    "df_2023['TaggedPitchType'].unique()\n",
    "#remove any Nan RunValue\n",
    "df_2022 = df_2022[df_2022['RunValue'].notna()]\n",
    "df_2023 = df_2023[df_2023['RunValue'].notna()]\n",
    "\n",
    "#change pitch type to match trackman data\n",
    "#FF, SI, FS,FA  = Fastball\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['FF','FA'],'Fastball')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['FF','FA'],'Fastball')\n",
    "\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['SI'], 'Sinker')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['SI'], 'Sinker')\n",
    "\n",
    "#SL,ST,FC  = Slider\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['SL'],'Slider')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['SL'],'Slider')\n",
    "\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['ST'],'Sweeper')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['ST'],'Sweeper')\n",
    "\n",
    "#CH, EP, SC, FO = ChangeUp\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['CH','EP','SC','FO','KN'],'ChangeUp')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['CH','EP','SC','FO','KN'],'ChangeUp')\n",
    "#SV, CU, KC = Curveball\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['SV','CU','KC'],'Curveball')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['SV','CU','KC'],'Curveball')\n",
    "#'FC' = Cutter\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['FC'],'Cutter')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['FC'],'Cutter')\n",
    "#SPL = FS\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['FS'],'Splitter')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['FS'],'Splitter')\n",
    "\n",
    "df_2022['ABS_Horizontal'] = abs(df_2022['HorzBreak'])\n",
    "df_2023['ABS_Horizontal'] = abs(df_2023['HorzBreak'])\n",
    "df_2022['ABS_RelSide'] = abs(df_2022['RelSide'])\n",
    "df_2023['ABS_RelSide'] = abs(df_2023['RelSide'])\n",
    "df_2022['differential_break'] = abs(df_2022['InducedVertBreak'] - df_2022['ABS_Horizontal'])\n",
    "df_2023['differential_break'] = abs(df_2023['InducedVertBreak'] - df_2023['ABS_Horizontal'])\n",
    "\n",
    "#for each pitcher in df_2022, df_2023, calculate differential_speed between the pitch with highest RelSpeed and any offspeed pitches they have - Offspeed\n",
    "\n",
    "\n",
    "#fastballs = 'Fastball', 'Sinker', 'TwoSeamFastBall', 'FourSeamFastBall', 'OneSeamFastBall'\n",
    "dfb2 = df_2022[df_2022.TaggedPitchType.isin(['Fastball', 'FourSeamFastball', 'OneSeamFastBall'])]\n",
    "dfb3 = df_2023[df_2023.TaggedPitchType.isin(['Fastball', 'FourSeamFastball', 'OneSeamFastBall'])]\n",
    "#'Sinker', 'TwoSeamFastBall',\n",
    "dsi2 = df_2022[df_2022.TaggedPitchType.isin(['Sinker', 'TwoSeamFastBall'])]\n",
    "dsi3 = df_2023[df_2023.TaggedPitchType.isin(['Sinker', 'TwoSeamFastBall'])]\n",
    "#sliders = 'Slider', 'Cutter'\n",
    "dsl2 = df_2022[df_2022.TaggedPitchType.isin(['Slider'])]\n",
    "dsl3 = df_2023[df_2023.TaggedPitchType.isin(['Slider'])]\n",
    "#Sweeper\n",
    "dst2 = df_2022[df_2022.TaggedPitchType.isin(['Sweeper'])]\n",
    "dst3 = df_2023[df_2023.TaggedPitchType.isin(['Sweeper'])]\n",
    "#curveballs = 'Curveball', 'KnuckleCurve'\n",
    "dcb2 = df_2022[df_2022.TaggedPitchType.isin(['Curveball', 'KnuckleCurve'])]\n",
    "dcb3 = df_2023[df_2023.TaggedPitchType.isin(['Curveball', 'KnuckleCurve'])]\n",
    "#changeups = 'Changeup', 'Splitter', 'Forkball', 'Screwball'\n",
    "dch2 = df_2022[df_2022.TaggedPitchType.isin(['ChangeUp'])]\n",
    "dch3 = df_2023[df_2023.TaggedPitchType.isin(['ChangeUp'])]\n",
    "#cutters = 'Cutter'\n",
    "dct2 = df_2022[df_2022.TaggedPitchType.isin(['Cutter'])]\n",
    "dct3 = df_2023[df_2023.TaggedPitchType.isin(['Cutter'])]\n",
    "#splitter\n",
    "dsp2 = df_2022[df_2022.TaggedPitchType.isin(['Splitter'])]\n",
    "dsp3 = df_2023[df_2023.TaggedPitchType.isin(['Splitter'])]\n",
    "#delete all rows with nan values\n",
    "dfb2 = dfb2.dropna()\n",
    "dsi2 = dsi2.dropna()\n",
    "dsl2 = dsl2.dropna()\n",
    "dst2 = dst2.dropna()\n",
    "dcb2 = dcb2.dropna()\n",
    "dch2 = dch2.dropna()\n",
    "dct2 = dct2.dropna()\n",
    "dsp2 = dsp2.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create X,y for each pitch, XGB model predicting Stuff+ for ech pitch, scale results to 100 average Stuff+ score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#xRV stuff\n",
    "X = dfb2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y = dfb2['RunValue']\n",
    "X1 = dsi2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y1 = dsi2['RunValue']\n",
    "X2 = dsl2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y2 = dsl2['RunValue']\n",
    "X3 = dst2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y3 = dst2['RunValue']\n",
    "X4 = dcb2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y4 = dcb2['RunValue']\n",
    "X5 = dch2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y5 = dch2['RunValue']\n",
    "X6 = dct2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y6 = dct2['RunValue']\n",
    "X7 = dsp2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y7 = dsp2['RunValue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.25, random_state=101)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.25, random_state=101)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.25, random_state=101)\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.25, random_state=101)\n",
    "\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size=0.25, random_state=101)\n",
    "\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, test_size=0.25, random_state=101)\n",
    "\n",
    "X7_train, X7_test, y7_train, y7_test = train_test_split(X7, y7, test_size=0.25, random_state=101)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "xgb_model_0 = XGBRegressor(**params)\n",
    "xgb_model_0.fit(X_train, y_train)\n",
    "\n",
    "#sinker\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1_train, y1_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "xgb_model1 = XGBRegressor(**params)\n",
    "xgb_model1.fit(X1_train, y1_train)\n",
    "\n",
    "#cslider\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_2 = XGBRegressor(**params)\n",
    "xgb_model_2.fit(X2_train, y2_train)\n",
    "\n",
    "#csweeper\n",
    "X3_train, X3_val, y3_train, y3_val = train_test_split(X3_train, y3_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_3 = XGBRegressor(**params)\n",
    "xgb_model_3.fit(X3_train, y3_train)\n",
    "\n",
    "#curveball\n",
    "X4_train, X4_val, y4_train, y4_val = train_test_split(X4_train, y4_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_4 = XGBRegressor(**params)\n",
    "xgb_model_4.fit(X4_train, y4_train)\n",
    "\n",
    "#changeup \n",
    "X5_train, X5_val, y5_train, y5_val = train_test_split(X5_train, y5_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_5 = XGBRegressor(**params)\n",
    "xgb_model_5.fit(X5_train, y5_train)\n",
    "\n",
    "#cutter\n",
    "X6_train, X6_val, y6_train, y6_val = train_test_split(X6_train, y6_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_6 = XGBRegressor(**params)\n",
    "xgb_model_6.fit(X6_train, y6_train)\n",
    "\n",
    "#splitter\n",
    "X7_train, X7_val, y7_train, y7_val = train_test_split(X7_train, y7_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_7 = XGBRegressor(**params)\n",
    "xgb_model_7.fit(X7_train, y7_train)\n",
    "dfb2['xRV_xgb']= xgb_model_0.predict(dfb2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsi2['xRV_xgb']= xgb_model1.predict(dsi2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsl2['xRV_xgb']= xgb_model_2.predict(dsl2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dst2['xRV_xgb']= xgb_model_3.predict(dst2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dcb2['xRV_xgb']= xgb_model_4.predict(dcb2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dch2['xRV_xgb']= xgb_model_5.predict(dch2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dct2['xRV_xgb']= xgb_model_6.predict(dct2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsp2['xRV_xgb']= xgb_model_7.predict(dsp2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dfb3['xRV_xgb']= xgb_model_0.predict(dfb3[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsi3['xRV_xgb']= xgb_model1.predict(dsi3[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsl3['xRV_xgb']= xgb_model_2.predict(dsl3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dst3['xRV_xgb']= xgb_model_3.predict(dst3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dcb3['xRV_xgb']= xgb_model_4.predict(dcb3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dch3['xRV_xgb']= xgb_model_5.predict(dch3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dct3['xRV_xgb']= xgb_model_6.predict(dct3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsp3['xRV_xgb']= xgb_model_7.predict(dsp3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "#find the r^2 for all the variables\n",
    "#create the FB expected whiff rate\n",
    "X = dfb2[['xRV_xgb']]\n",
    "y = dfb2['RunValue']\n",
    "X2 = dsi2[['xRV_xgb']]\n",
    "y2 = dsi2['RunValue']\n",
    "X3 = dsl2[['xRV_xgb']]\n",
    "y3 = dsl2['RunValue']\n",
    "X4 = dst2[['xRV_xgb']]\n",
    "y4 = dst2['RunValue']\n",
    "X5 = dcb2[['xRV_xgb']]\n",
    "y5 = dcb2['RunValue']\n",
    "X6 = dch2[['xRV_xgb']]\n",
    "y6 = dch2['RunValue']\n",
    "X7 = dct2[['xRV_xgb']]\n",
    "y7 = dct2['RunValue']\n",
    "X8 = dsp2[['xRV_xgb']]\n",
    "y8 = dsp2['RunValue']\n",
    "dfb2_max = dfb2['xRV_xgb'].max()\n",
    "dsi2_max = dsi2['xRV_xgb'].max()\n",
    "dsl2_max = dsl2['xRV_xgb'].max()\n",
    "dst2_max = dst2['xRV_xgb'].max()\n",
    "dcb2_max = dcb2['xRV_xgb'].max()\n",
    "dch2_max = dch2['xRV_xgb'].max()\n",
    "dct2_max = dct2['xRV_xgb'].max()\n",
    "dsp2_max = dsp2['xRV_xgb'].max()\n",
    "dfb3_max = dfb3['xRV_xgb'].max()\n",
    "dsi3_max = dsi3['xRV_xgb'].max()\n",
    "dsl3_max = dsl3['xRV_xgb'].max()\n",
    "dst3_max = dst3['xRV_xgb'].max()\n",
    "dcb3_max = dcb3['xRV_xgb'].max()\n",
    "dch3_max = dch3['xRV_xgb'].max()\n",
    "dct3_max = dct3['xRV_xgb'].max()\n",
    "dsp3_max = dsp3['xRV_xgb'].max()\n",
    "dfb2_max = dfb2['xRV_xgb'].max()\n",
    "dsi2_max = dsi2['xRV_xgb'].max()\n",
    "dsl2_max = dsl2['xRV_xgb'].max()\n",
    "dst2_max = dst2['xRV_xgb'].max()\n",
    "dcb2_max = dcb2['xRV_xgb'].max()\n",
    "dch2_max = dch2['xRV_xgb'].max()\n",
    "dct2_max = dct2['xRV_xgb'].max()\n",
    "dsp2_max = dsp2['xRV_xgb'].max()\n",
    "dfb3_max = dfb3['xRV_xgb'].max()\n",
    "dsi3_max = dsi3['xRV_xgb'].max()\n",
    "dsl3_max = dsl3['xRV_xgb'].max()\n",
    "dst3_max = dst3['xRV_xgb'].max()\n",
    "dcb3_max = dcb3['xRV_xgb'].max()\n",
    "dch3_max = dch3['xRV_xgb'].max()\n",
    "dct3_max = dct3['xRV_xgb'].max()\n",
    "dsp3_max = dsp3['xRV_xgb'].max()\n",
    "\n",
    "#scaled \n",
    "dfb2['xRV_Scaled'] = dfb2['xRV_xgb'] - dfb2_max\n",
    "dfb2['xRV/100_stuff_scaled_abs'] = abs(dfb2['xRV_Scaled'])\n",
    "dfb2['Stuff_plus'] = dfb2['xRV/100_stuff_scaled_abs'] / dfb2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dfb3['xRV_Scaled'] = dfb3['xRV_xgb'] - dfb3_max\n",
    "dfb3['xRV/100_stuff_scaled_abs'] = abs(dfb3['xRV_Scaled'])\n",
    "dfb3['Stuff_plus'] = dfb3['xRV/100_stuff_scaled_abs'] / dfb3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dsi2['xRV_Scaled'] = dsi2['xRV_xgb'] - dsi2_max\n",
    "dsi2['xRV/100_stuff_scaled_abs'] = abs(dsi2['xRV_Scaled'])\n",
    "dsi2['Stuff_plus'] = dsi2['xRV/100_stuff_scaled_abs'] / dsi2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dsi3['xRV_Scaled'] = dsi3['xRV_xgb'] - dsi3_max\n",
    "dsi3['xRV/100_stuff_scaled_abs'] = abs(dsi3['xRV_Scaled'])\n",
    "dsi3['Stuff_plus'] = dsi3['xRV/100_stuff_scaled_abs'] / dsi3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dsl2['xRV_Scaled'] = dsl2['xRV_xgb'] - dfb2_max\n",
    "dsl2['xRV/100_stuff_scaled_abs'] = abs(dsl2['xRV_Scaled'])\n",
    "dsl2['Stuff_plus'] = dsl2['xRV/100_stuff_scaled_abs'] / dsl2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dsl3['xRV_Scaled'] = dsl3['xRV_xgb'] - dsl3_max\n",
    "dsl3['xRV/100_stuff_scaled_abs'] = abs(dsl3['xRV_Scaled'])\n",
    "dsl3['Stuff_plus'] = dsl3['xRV/100_stuff_scaled_abs'] / dsl3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dst2['xRV_Scaled'] = dst2['xRV_xgb'] - dst2_max\n",
    "dst2['xRV/100_stuff_scaled_abs'] = abs(dst2['xRV_Scaled'])\n",
    "dst2['Stuff_plus'] = dst2['xRV/100_stuff_scaled_abs'] / dst2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dst3['xRV_Scaled'] = dst3['xRV_xgb'] - dst3_max\n",
    "dst3['xRV/100_stuff_scaled_abs'] = abs(dst3['xRV_Scaled'])\n",
    "dst3['Stuff_plus'] = dst3['xRV/100_stuff_scaled_abs'] / dst3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dcb2['xRV_Scaled'] = dcb2['xRV_xgb'] - dcb2_max\n",
    "dcb2['xRV/100_stuff_scaled_abs'] = abs(dcb2['xRV_Scaled'])\n",
    "dcb2['Stuff_plus'] = dcb2['xRV/100_stuff_scaled_abs'] / dcb2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dcb3['xRV_Scaled'] = dcb3['xRV_xgb'] - dcb3_max\n",
    "dcb3['xRV/100_stuff_scaled_abs'] = abs(dcb3['xRV_Scaled'])\n",
    "dcb3['Stuff_plus'] = dcb3['xRV/100_stuff_scaled_abs'] / dcb3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dch2['xRV_Scaled'] = dch2['xRV_xgb'] - dch2_max\n",
    "dch2['xRV/100_stuff_scaled_abs'] = abs(dch2['xRV_Scaled'])\n",
    "dch2['Stuff_plus'] = dch2['xRV/100_stuff_scaled_abs'] / dch2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dch3['xRV_Scaled'] = dch3['xRV_xgb'] - dch3_max\n",
    "dch3['xRV/100_stuff_scaled_abs'] = abs(dch3['xRV_Scaled'])\n",
    "dch3['Stuff_plus'] = dch3['xRV/100_stuff_scaled_abs'] / dch3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dct2['xRV_Scaled'] = dct2['xRV_xgb'] - dct2_max\n",
    "dct2['xRV/100_stuff_scaled_abs'] = abs(dct2['xRV_Scaled'])\n",
    "dct2['Stuff_plus'] = dct2['xRV/100_stuff_scaled_abs'] / dct2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dct3['xRV_Scaled'] = dct3['xRV_xgb'] - dct3_max\n",
    "dct3['xRV/100_stuff_scaled_abs'] = abs(dct3['xRV_Scaled'])\n",
    "dct3['Stuff_plus'] = dct3['xRV/100_stuff_scaled_abs'] / dct3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#splitter\n",
    "dsp2['xRV_Scaled'] = dsp2['xRV_xgb'] - dsp2_max\n",
    "dsp2['xRV/100_stuff_scaled_abs'] = abs(dsp2['xRV_Scaled'])\n",
    "dsp2['Stuff_plus'] = dsp2['xRV/100_stuff_scaled_abs'] / dsp2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dsp3['xRV_Scaled'] = dsp3['xRV_xgb'] - dsp3_max\n",
    "dsp3['xRV/100_stuff_scaled_abs'] = abs(dsp3['xRV_Scaled'])\n",
    "dsp3['Stuff_plus'] = dsp3['xRV/100_stuff_scaled_abs'] / dsp3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Scaling to ensure Stuff+ ~ N(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df_total is your DataFrame and 'Stuff_plus' is the name of your column\n",
    "current_std_fb = dfb3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_fb\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dfb3['Stuff_plus'].mean()\n",
    "dfb3['Final_Adjusted_Stuff_Plus'] = ((dfb3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dfb3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dfb3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_si = dsi3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_si\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dsi3['Stuff_plus'].mean()\n",
    "dsi3['Final_Adjusted_Stuff_Plus'] = ((dsi3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dsi3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dsi3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_sl = dsl3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_sl\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dsl3['Stuff_plus'].mean()\n",
    "dsl3['Final_Adjusted_Stuff_Plus'] = ((dsl3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dsl3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dsl3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_st = dst3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_st\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dst3['Stuff_plus'].mean()\n",
    "dst3['Final_Adjusted_Stuff_Plus'] = ((dst3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dst3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dst3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_cb = dcb3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_cb\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dcb3['Stuff_plus'].mean()\n",
    "dcb3['Final_Adjusted_Stuff_Plus'] = ((dcb3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dcb3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dcb3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_ch = dch3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_ch\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dch3['Stuff_plus'].mean()\n",
    "dch3['Final_Adjusted_Stuff_Plus'] = ((dch3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dch3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dch3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_ct = dct3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_ch\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dct3['Stuff_plus'].mean()\n",
    "dct3['Final_Adjusted_Stuff_Plus'] = ((dct3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dct3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dct3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "#splitter\n",
    "current_std_sp = dsp3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_sp\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dsp3['Stuff_plus'].mean()\n",
    "dsp3['Final_Adjusted_Stuff_Plus'] = ((dsp3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "#combine all the dataframes\n",
    "df_2022 = pd.concat([dfb2, dsi2, dsl2, dst2, dcb2, dch2, dct2, dsp2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb3_grouped = dfb3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',  # or 'last', depending on which one you want\n",
    "    'Pitcher_ID': 'first',  # or 'last', depending on which one you want\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'differential_break': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dsi3_grouped = dsi3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'differential_break': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dsl3_grouped = dsl3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dst3_grouped = dst3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dcb3_grouped = dcb3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dch3_grouped = dch3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dct3_grouped = dct3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "    \n",
    "\n",
    "dsp3_grouped = dsp3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_total = pd.concat([dfb3, dsi3, dsl3, dst3, dcb3, dch3, dct3, dsp3], ignore_index=True)\n",
    "#groupby pitcher and TaggedPitchType\n",
    "df_2023_total_pitch_RV = df_2023_total.groupby(['Pitcher','Pitcher_ID','PitcherTeam','TaggedPitchType']).agg({'RelSpeed': 'mean','InducedVertBreak':'mean','HorzBreak':'mean','RelHeight':'mean','ABS_RelSide': 'mean', 'Extension':'mean','Final_Adjusted_Stuff_Plus': 'mean', 'RunValue': 'sum', 'launch_speed':'mean', 'bat_speed':'mean'}).reset_index()\n",
    "df_2023_total_pitcher_RV = df_2023_total.groupby(['Pitcher','Pitcher_ID','PitcherTeam']).agg({'Final_Adjusted_Stuff_Plus': 'mean', 'RunValue': 'sum', 'launch_speed':'mean','bat_speed':'mean'}).reset_index()\n",
    "df_2023_total_pitcher_RV_release = df_2023_total.groupby(['Pitcher']).agg({'RelHeight':'mean','ABS_RelSide': 'mean', 'Extension':'mean'}).reset_index()\n",
    "df_2023_total_pitcher_RV['Count'] = df_2023_total.groupby(['Pitcher']).size().reset_index(name='Count')['Count']\n",
    "top_25_all = df_2023_total_pitcher_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "#top_25_count_50  over 50 pitches\n",
    "top_25_count_50 = df_2023_total_pitcher_RV[df_2023_total_pitcher_RV['Count'] > 50].sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "top_pitches = df_2023_total_pitch_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Pitcher IDs: <IntegerArray>\n",
      "[664126, 656271, 670955, 690829, 669358, 543294, 676508, 687377, 680736,\n",
      " 695418,\n",
      " ...\n",
      " 702352, 657649, 445926, 664208, 445276, 669203, 687765, 677649, 668933,\n",
      " 689254]\n",
      "Length: 427, dtype: Int64\n",
      "Pitcher Mapping: {664126: 'Fairbanks, Pete', 656271: 'Burke, Brock', 670955: 'Uceta, Edwin', 690829: 'Joyce, Ben', 669358: 'Baz, Shane', 543294: 'Hendricks, Kyle', 676508: 'Casparius, Ben', 687377: 'Ribalta, Orlando', 680736: 'Wrobleski, Justin', 695418: 'Lord, Brad', 678215: 'Arias, Luarbert', 675540: 'Curry, Xzavion', 623211: 'Brazobán, Huascar', 687362: 'Gillispie, Connor', 605280: 'Holmes, Clay', 628452: 'Iglesias, Raisel', 621383: 'Banks, Tanner', 689147: 'Kerkering, Orion', 669276: 'Lee, Dylan', 554430: 'Wheeler, Zack', 660853: 'De Los Santos, Enyel', 519242: 'Sale, Chris', 606930: 'Barnes, Jacob', 684320: 'Rodríguez, Yariel', 643338: 'Green, Chad', 676979: 'Crochet, Garrett', 687922: 'Lucas, Easton', 669373: 'Skubal, Tarik', 471911: 'Carrasco, Carlos', 605182: 'Clevinger, Mike', 656457: 'Gilbert, Tyler', 623149: 'Sewald, Paul', 681343: 'Smith, Shane', 671922: 'Smith, Cade', 594902: 'Lively, Ben', 519151: 'Pressly, Ryan', 455119: 'Martin, Chris', 687863: 'Hodge, Porter', 657240: 'Merryweather, Julian', 687847: 'Church, Marc', 592791: 'Taillon, Jameson', 657097: 'Webb, Jacob', 643377: 'Jax, Griffin', 670167: 'Schreiber, John', 672582: 'Zerpa, Angel', 666142: 'Ragans, Cole', 641154: 'López, Pablo', 595345: 'Okert, Steven', 687473: 'Gusto, Ryan', 662253: 'Muñoz, Andrés', 687911: 'King, Bryan', 622491: 'Castillo, Luis', 641941: 'Pagán, Emilio', 806185: 'Birdsong, Hayden', 666157: 'Lodolo, Nick', 640462: 'Puk, A.J.', 669211: 'Akin, Keegan', 571946: 'Miller, Shelby', 518876: 'Kelly, Merrill', 450203: 'Morton, Charlie', 621053: 'Ferguson, Tyler', 686993: 'Sterner, Justin', 673513: 'Matsui, Yuki', 600917: 'Leclerc, José', 689690: 'Jacob, Alek', 605488: 'Springs, Jeffrey', 656302: 'Cease, Dylan', 683409: 'Chivilli, Angel', 683232: 'Mears, Nick', 606303: 'Payamps, Joel', 623474: 'Herget, Jimmy', 642547: 'Peralta, Freddy', 607536: 'Freeland, Kyle', 656546: 'Hoffman, Jeff', 554340: 'García, Yimi', 669711: 'Weissert, Greg', 621244: 'Berríos, José', 690916: 'Fitts, Richard', 689225: 'Brieske, Beau', 607074: 'Rodón, Carlos', 663554: 'Mize, Casey', 640448: 'Finnegan, Kyle', 608371: 'Sims, Lucas', 607455: 'Banda, Anthony', 669022: 'Gore, MacKenzie', 669160: 'May, Dustin', 592773: 'Stanek, Ryne', 666277: 'Soriano, George', 685107: 'Veneziano, Anthony', 669199: 'Bachar, Lake', 673540: 'Senga, Kodai', 678368: 'Bellozo, Valente', 802419: 'Harrington, Thomas', 688297: 'Roycroft, Chris', 669461: 'Liberatore, Matthew', 669387: 'Mlodzinski, Carmen', 663158: 'Suarez, Robert', 592094: 'Adam, Jason', 622780: 'Perdomo, Angel', 669093: 'Estrada, Jeremiah', 650633: 'King, Michael', 622663: 'Severino, Luis', 669194: 'Nelson, Ryne', 621107: 'Eflin, Zach', 668678: 'Gallen, Zac', 608032: 'Estévez, Carlos', 656240: 'Blewett, Scott', 640451: 'Harvey, Hunter', 680573: 'Woods Richardson, Simeon', 547179: 'Lorenzen, Michael', 607067: 'Rea, Colin', 657006: 'Steele, Justin', 642520: 'Garabito, Gerson', 543135: 'Eovaldi, Nathan', 663574: 'Santillan, Tony', 668881: 'Greene, Hunter', 642152: 'Trivino, Lou', 657277: 'Webb, Logan', 650556: 'Abreu, Bryan', 605463: 'Scott, Tayler', 669713: 'Wesneski, Hayden', 669302: 'Gilbert, Logan', 681676: 'Fernandez, Ryan', 547973: 'Chapman, Aroldis', 686580: 'Slaten, Justin', 458677: 'Wilson, Justin', 669467: 'Pallante, Andre', 656794: 'Newcomb, Sean', 621242: 'Díaz, Edwin', 621345: 'Minter, A.J.', 670102: 'Francis, Bowden', 656849: 'Peterson, David', 605447: 'Romano, Jordan', 614179: 'Ruiz, José', 676263: 'Dreyer, Jack', 681911: 'Vesia, Alex', 673929: 'Leasure, Jordan', 605154: 'Brebbia, John', 527048: 'Pérez, Martín', 695549: 'Jobe, Jackson', 700669: 'Graceffo, Gordon', 690928: 'Dobbins, Hunter', 571945: 'Mikolas, Miles', 676962: 'Brown, Ben', 670810: 'Gillaspie, Logan', 656222: 'Beeks, Jalen', 592866: 'Williams, Trevor', 666214: 'Wentz, Joey', 596133: 'Weaver, Luke', 657571: 'Ferguson, Caleb', 642701: 'Santana, Dennis', 687396: 'Headrick, Brent', 571760: 'Heaney, Andrew', 701542: 'Warren, Will', 669062: 'Miller, Erik', 693433: 'Woo, Bryan', 682847: 'Ortiz, Luis L.', 542881: 'Anderson, Tyler', 680767: 'Vodnik, Victor', 678821: 'Peralta, Luis', 801403: 'Dollander, Chase', 683155: 'Estes, Joey', 663460: 'Bubic, Kris', 700249: 'Povich, Cade', 664139: 'Gibaut, Ian', 608718: 'Suter, Brent', 694477: 'Patrick, Chad', 594580: 'Moll, Sam', 686730: 'Spiers, Carson', 655889: 'Rodríguez, Manuel', 571948: 'Milner, Hoby', 656876: 'Rasmussen, Drew', 677958: 'Rocker, Kumar', 686973: 'Varland, Louis', 606160: 'Montero, Rafael', 663978: 'Paddack, Chris', 664351: 'Contreras, Luis', 669854: 'Blanco, Ronel', 680730: 'Parker, Mitchell', 593958: 'Rodriguez, Eduardo', 660761: 'Suarez, José', 700363: 'Smith-Shawver, AJ', 615698: 'Quantrill, Cal', 573204: 'Thielbar, Caleb', 663878: 'Pearson, Nate', 678316: 'Cruz, Omar', 571510: 'Boyd, Matthew', 601713: 'Pivetta, Nick', 518585: 'Cruz, Fernando', 670059: 'Holderman, Colin', 663559: 'Falter, Bailey', 656945: 'Scott, Tanner', 605452: 'Ross, Joe', 808963: 'Sasaki, Roki', 605400: 'Nola, Aaron', 676130: 'Buttó, José', 605135: 'Bassitt, Chris', 656288: 'Canning, Griffin', 681857: 'Olson, Reese', 663436: 'Martin, Davis', 695243: 'Miller, Mason', 687134: 'Blalock, Bradley', 676664: 'Sears, JP', 608566: 'Márquez, Germán', 663738: 'Lynch IV, Daniel', 608717: 'Stratton, Chris', 608372: 'Sugano, Tomoyuki', 669674: 'Long, Sam', 608379: 'Wacha, Michael', 661395: 'Duran, Jhoan', 660896: 'Alcala, Jorge', 681293: 'Arrighetti, Spencer', 641927: 'Ober, Bailey', 605130: 'Barlow, Scott', 660730: 'Rodriguez, Elvin', 663903: 'Singer, Brady', 592426: 'Jackson, Luke', 682254: 'Montgomery, Mason', 594798: 'deGrom, Jacob', 671737: 'Bradley, Taj', 676282: 'Cantillo, Joey', 666120: 'Anderson, Ian', 663474: 'McKenzie, Triston', 676440: 'Bibee, Tanner', 686799: 'Kochanowicz, Jack', 682243: 'Miller, Bryce', 592662: 'Ray, Robbie', 668820: 'Kranick, Max', 592332: 'Gausman, Kevin', 656731: 'Megill, Tylor', 607481: 'Bummer, Aaron', 691594: 'Sanoja, Javier', 680885: 'Schwellenbach, Spencer', 676974: 'Meyer, Max', 677161: 'Kelly, Zack', 621111: 'Buehler, Walker', 642207: 'Williams, Devin', 608331: 'Fried, Max', 656605: 'Keller, Mitch', 684007: 'Imanaga, Shota', 621381: 'Strahm, Matt', 489446: 'Yates, Kirby', 666200: 'Luzardo, Jesús', 808967: 'Yamamoto, Yoshinobu', 676684: 'Vest, Will', 669060: 'Wilson, Bryse', 656427: 'Flaherty, Jack', 685126: 'Eisert, Brandon', 686563: 'Cannon, Jonathan', 679885: 'Martinez, Justin', 621363: 'Poche, Colin', 694297: 'Pfaadt, Brandon', 663623: 'Irvin, Jake', 678020: 'Halvorsen, Seth', 663372: 'Feltner, Ryan', 674370: 'Bido, Osvaldo', 672335: 'Pérez, Cionel', 668674: 'Erceg, Lucas', 622554: 'Domínguez, Seranthony', 607625: 'Lugo, Seth', 665152: 'Kremer, Dean', 685801: 'Bigge, Hunter', 542888: 'Armstrong, Shawn', 641793: 'Littell, Zack', 641816: 'Mahle, Tyler', 656730: 'Megill, Trevor', 663542: 'Hudson, Bryan', 641302: 'Alexander, Tyler', 607259: 'Martinez, Nick', 702674: 'Dana, Caden', 683769: 'Gaddis, Hunter', 666171: 'Zeferjahn, Ryan', 682120: 'Herrin, Tim', 667755: 'Soriano, José', 668909: 'Williams, Gavin', 660825: 'Bazardo, Eduard', 676092: 'Snider, Collin', 622379: 'Castillo, Luis F.', 434378: 'Verlander, Justin', 641329: 'Baker, Bryan', 656557: 'Houck, Tanner', 641482: 'Cortes, Nestor', 641755: 'Kinley, Tyler', 622608: 'Senzatela, Antonio', 592836: 'Walker, Taijuan', 686613: 'Brown, Hunter', 657746: 'Ryan, Joe', 672782: 'Gómez, Yoendrys', 676534: 'Faucher, Calvin', 683004: 'Leiter, Jack', 694973: 'Skenes, Paul', 686752: 'Pepiot, Ryan', 592155: 'Booser, Cam', 680732: 'Burke, Sean', 579328: 'Kikuchi, Yusei', 543243: 'Gray, Sonny', 678226: 'Hernández, Daysbel', 693821: 'Elder, Bryce', 605483: 'Snell, Blake', 669212: 'Morgan, Eli', 678692: 'Henriquez, Ronny', 645261: 'Alcantara, Sandy', 664854: 'Helsley, Ryan', 664875: 'Lawrence, Justin', 605347: 'López, Jorge', 686642: 'Ellard, Fraser', 663485: 'Sands, Cole', 682171: 'Murfee, Penn', 646242: 'Díaz, Jhonathan', 663423: 'Thornton, Trent', 671106: 'Allen, Logan', 669438: 'Englert, Mason', 674285: 'Salazar, Eduardo', 647336: 'Soroka, Michael', 670766: 'McCaughan, Darren', 672282: 'Detmers, Reid', 472610: 'García, Luis', 607192: 'Glasnow, Tyler', 656550: 'Holmes, Grant', 593974: 'Peralta, Wandy', 606996: 'Hart, Kyle', 641343: 'Bauers, Jake', 650644: 'Civale, Aaron', 677976: 'Dobnak, Randy', 593576: 'Neris, Héctor', 671162: 'Thomas, Connor', 687330: 'Kelly, Kevin', 664076: 'Cleavinger, Garrett', 663992: 'Lovelady, Richard', 453286: 'Scherzer, Max', 672578: 'Hernández, Carlos', 628317: 'Maeda, Kenta', 676395: 'Garcia, Robert', 668941: 'Romero, JoJo', 681190: 'Vásquez, Randy', 544150: 'Suárez, Albert', 670280: 'Bednar, David', 641745: 'Keller, Brad', 625643: 'López, Reynaldo', 678495: 'Rodríguez, Randy', 680704: 'Sandlin, Nick', 670032: 'Lopez, Nicky', 696270: 'Johnson, Ryan', 686826: 'Jarvis, Bryce', 681517: 'Leahy, Kyle', 671131: 'Rutledge, Jackson', 657585: 'Garrett, Reed', 670174: 'Winckowski, Josh', 676428: 'Hurter, Brant', 641656: 'Hamilton, Ian', 642232: 'Yarbrough, Ryan', 571578: 'Corbin, Patrick', 543056: 'Coulombe, Danny', 642100: 'Speier, Gabe', 664285: 'Valdez, Framber', 476594: 'Stock, Robert', 592454: 'Kahnle, Tommy', 493603: 'Ottavino, Adam', 663947: 'Holton, Tyler', 669422: 'Sauer, Matt', 663969: 'Phillips, Tyler', 642397: 'Soto, Gregory', 666974: 'Cano, Yennier', 657044: 'Thompson, Ryan', 666619: 'Santos, Gregory', 595014: 'Treinen, Blake', 667463: 'King, John', 670970: 'Morejon, Adrian', 663855: 'Hicks, Jordan', 519008: 'McFarland, T.J.', 681982: 'Anderson, Grant', 573009: 'Mantiply, Joe', 669622: 'Bender, Anthony', 657612: 'Hill, Tim', 573186: 'Stroman, Marcus', 661403: 'Clase, Emmanuel', 596001: 'Junis, Jakob', 678024: 'Vasil, Mike', 572955: 'Johnson, Pierce', 669724: 'Hanifee, Brenan', 650489: 'Castro, Willi', 621237: 'Alvarado, José', 640902: 'Pereda, Jhonny', 668716: 'Murdock, Noah', 672841: 'Vargas, Carlos', 676106: 'Hancock, Emerson', 663687: 'Harris, Hogan', 642048: 'Saucedo, Tayler', 622786: 'Tinoco, Jesus', 664849: 'Young, Danny', 534910: 'Hahn, Jesse', 623352: 'Hader, Josh', 694738: 'Roupp, Landen', 682842: 'Uribe, Abner', 642585: 'Bautista, Félix', 676477: 'Whitlock, Garrett', 663893: 'Little, Brendon', 650911: 'Sánchez, Cristopher', 681867: 'Criswell, Cooper', 657514: 'Bernardino, Brennan', 678606: 'Ferrer, Jose A.', 663767: 'Shugart, Chase', 621366: 'Borucki, Ryan', 666808: 'Doval, Camilo', 643511: 'Rogers, Tyler', 623437: 'Topa, Justin', 643410: 'Leiter Jr., Mark', 656234: 'Bird, Jake', 518397: 'Alexander, Scott', 621199: 'Bowman, Matt', 665625: 'Peguero, Elvis', 573124: 'Rogers, Taylor', 680729: 'McDaniels, Garrett', 676254: 'Walker, Ryan', 571927: 'Matz, Steven', 607200: 'Fedde, Erick', 641835: 'Mayza, Tim', 702352: 'Bivens, Spencer', 657649: 'Koenig, Jared', 445926: 'Chavez, Jesse', 664208: 'Maton, Phil', 445276: 'Jansen, Kenley', 669203: 'Burnes, Corbin', 687765: 'Spence, Mitch', 677649: 'Duran, Ezequiel', 668933: 'Ashcraft, Graham', 689254: 'Fluharty, Mason'}\n",
      "\n",
      "2025 Pitcher Stats DataFrame:\n",
      "     Pitcher_ID           Pitcher    IP TBF  WHIP   ERA   FIP     K%    BB%  \\\n",
      "0        664126   Fairbanks, Pete   5.0  21  1.40  1.80  2.01  33.3%  14.3%   \n",
      "1        656271      Burke, Brock   4.1  23  2.08  8.31  6.47  21.7%  13.0%   \n",
      "2        670955      Uceta, Edwin   4.2  22  1.50  5.79  5.36   9.1%  13.6%   \n",
      "3        690829        Joyce, Ben   4.1  20  1.38  6.23  6.24   5.0%   5.0%   \n",
      "4        669358        Baz, Shane  13.0  51  1.08  1.38  2.47  31.4%   7.8%   \n",
      "..          ...               ...   ...  ..   ...   ...   ...    ...    ...   \n",
      "422      669203    Burnes, Corbin   9.1  47  1.93  5.79  6.33  23.4%  14.9%   \n",
      "423      687765     Spence, Mitch   9.0  42  1.67  8.00  2.23  31.0%   4.8%   \n",
      "424      677649   Duran, Ezequiel   1.0   3  0.00  0.00  3.01   0.0%   0.0%   \n",
      "425      668933  Ashcraft, Graham   5.1  18  0.94  0.00  2.44  16.7%   5.6%   \n",
      "426      689254   Fluharty, Mason   2.0  10  2.00  9.00  2.51  20.0%  10.0%   \n",
      "\n",
      "     K-BB%  \n",
      "0    19.0%  \n",
      "1     8.7%  \n",
      "2    -4.5%  \n",
      "3     0.0%  \n",
      "4    23.5%  \n",
      "..     ...  \n",
      "422   8.5%  \n",
      "423  26.2%  \n",
      "424   0.0%  \n",
      "425  11.1%  \n",
      "426  10.0%  \n",
      "\n",
      "[427 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fangraphs_pitching_leaderboards(season: int):\n",
    "    url = f\"https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&season={season}&season1={season}&ind=0&qual=0&type=8&month=0&pageitems=500000\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for season {season}. Status code: {response.status_code}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data=data['data'])\n",
    "    return df\n",
    "\n",
    "# Step 1: Extract unique Pitcher_ID values\n",
    "unique_pitcher_ids = df_2023_total['Pitcher_ID'].unique()\n",
    "print(\"Unique Pitcher IDs:\", unique_pitcher_ids)\n",
    "\n",
    "# Step 2: Create a mapping of Pitcher_ID to Pitcher name (using the first occurrence)\n",
    "pitcher_mapping = df_2023_total.drop_duplicates(subset=['Pitcher_ID'])[['Pitcher_ID', 'Pitcher']].set_index('Pitcher_ID')['Pitcher'].to_dict()\n",
    "print(\"Pitcher Mapping:\", pitcher_mapping)\n",
    "\n",
    "# Step 3: Function to fetch and format stats for a given Pitcher_ID\n",
    "def get_fangraphs_pitcher_stats(pitcher_id: int, season: int, stats: list):\n",
    "    df_fangraphs = fangraphs_pitching_leaderboards(season=season)\n",
    "    \n",
    "    if df_fangraphs.empty:\n",
    "        print(f\"No data available for season {season}\")\n",
    "        return None\n",
    "    \n",
    "    df_fangraphs_pitcher = df_fangraphs[df_fangraphs['xMLBAMID'] == pitcher_id][stats].reset_index(drop=True)\n",
    "    \n",
    "    if df_fangraphs_pitcher.empty:\n",
    "        print(f\"No stats found for pitcher with ID {pitcher_id} in {season}\")\n",
    "        return None\n",
    "    \n",
    "    # Format the values using fangraphs_stats_dict\n",
    "    formatted_stats = {}\n",
    "    for stat in stats:\n",
    "        value = df_fangraphs_pitcher[stat].iloc[0]\n",
    "        formatted_stats[stat] = format(value, fangraphs_stats_dict.get(stat, {'format': '.2f'})['format']) if value != '---' else '---'\n",
    "    \n",
    "    return {\n",
    "        'Pitcher_ID': pitcher_id,\n",
    "        'Pitcher': pitcher_mapping.get(pitcher_id, 'Unknown'),\n",
    "        **formatted_stats\n",
    "    }\n",
    "\n",
    "# FANGRAPHS STATS DICT (same as provided)\n",
    "fangraphs_stats_dict = {\n",
    "    'IP': {'table_header': '$\\\\bf{IP}$', 'format': '.1f'},\n",
    "    'TBF': {'table_header': '$\\\\bf{PA}$', 'format': '.0f'},\n",
    "    'AVG': {'table_header': '$\\\\bf{AVG}$', 'format': '.3f'},\n",
    "    'K/9': {'table_header': '$\\\\bf{K/9}$', 'format': '.2f'},\n",
    "    'BB/9': {'table_header': '$\\\\bf{BB/9}$', 'format': '.2f'},\n",
    "    'K/BB': {'table_header': '$\\\\bf{K/BB}$', 'format': '.2f'},\n",
    "    'HR/9': {'table_header': '$\\\\bf{HR/9}$', 'format': '.2f'},\n",
    "    'K%': {'table_header': '$\\\\bf{K\\\\%}$', 'format': '.1%'},\n",
    "    'BB%': {'table_header': '$\\\\bf{BB\\\\%}$', 'format': '.1%'},\n",
    "    'K-BB%': {'table_header': '$\\\\bf{K-BB\\\\%}$', 'format': '.1%'},\n",
    "    'WHIP': {'table_header': '$\\\\bf{WHIP}$', 'format': '.2f'},\n",
    "    'BABIP': {'table_header': '$\\\\bf{BABIP}$', 'format': '.3f'},\n",
    "    'LOB%': {'table_header': '$\\\\bf{LOB\\\\%}$', 'format': '.1%'},\n",
    "    'xFIP': {'table_header': '$\\\\bf{xFIP}$', 'format': '.2f'},\n",
    "    'FIP': {'table_header': '$\\\\bf{FIP}$', 'format': '.2f'},\n",
    "    'H': {'table_header': '$\\\\bf{H}$', 'format': '.0f'},\n",
    "    '2B': {'table_header': '$\\\\bf{2B}$', 'format': '.0f'},\n",
    "    '3B': {'table_header': '$\\\\bf{3B}$', 'format': '.0f'},\n",
    "    'R': {'table_header': '$\\\\bf{R}$', 'format': '.0f'},\n",
    "    'ER': {'table_header': '$\\\\bf{ER}$', 'format': '.0f'},\n",
    "    'HR': {'table_header': '$\\\\bf{HR}$', 'format': '.0f'},\n",
    "    'BB': {'table_header': '$\\\\bf{BB}$', 'format': '.0f'},\n",
    "    'IBB': {'table_header': '$\\\\bf{IBB}$', 'format': '.0f'},\n",
    "    'HBP': {'table_header': '$\\\\bf{HBP}$', 'format': '.0f'},\n",
    "    'SO': {'table_header': '$\\\\bf{SO}$', 'format': '.0f'},\n",
    "    'OBP': {'table_header': '$\\\\bf{OBP}$', 'format': '.0f'},\n",
    "    'SLG': {'table_header': '$\\\\bf{SLG}$', 'format': '.0f'},\n",
    "    'ERA': {'table_header': '$\\\\bf{ERA}$', 'format': '.2f'},\n",
    "    'wOBA': {'table_header': '$\\\\bf{wOBA}$', 'format': '.3f'},\n",
    "    'G': {'table_header': '$\\\\bf{G}$', 'format': '.0f'}\n",
    "}\n",
    "\n",
    "# Step 4: Define stats to fetch and fetch stats for each unique Pitcher_ID\n",
    "selected_stats = ['IP', 'TBF', 'WHIP', 'ERA', 'FIP', 'K%', 'BB%', 'K-BB%']\n",
    "stats_list = []\n",
    "\n",
    "for pitcher_id in unique_pitcher_ids:\n",
    "    stats = get_fangraphs_pitcher_stats(pitcher_id, season=2025, stats=selected_stats)\n",
    "    if stats is not None:\n",
    "        stats_list.append(stats)\n",
    "\n",
    "# Step 5: Create a DataFrame from the collected stats\n",
    "if stats_list:\n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    print(\"\\n2025 Pitcher Stats DataFrame:\")\n",
    "    print(df_stats)\n",
    "else:\n",
    "    print(\"No valid stats were retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df_2023_total_pitcher_RV and df_stats on 'Pitcher_ID'\n",
    "df_2023_total_pitcher_RV = df_2023_total_pitcher_RV.merge(df_stats, on='Pitcher_ID', how='left', suffixes=('', '_fangraphs'))\n",
    "df_2023_total_pitcher_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_total_pitcher_RV_count100 = df_2023_total_pitcher_RV[df_2023_total_pitcher_RV['Count'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               RunValue   R-squared:                       0.051\n",
      "Model:                            OLS   Adj. R-squared:                  0.046\n",
      "Method:                 Least Squares   F-statistic:                     9.809\n",
      "Date:                Thu, 10 Apr 2025   Prob (F-statistic):            0.00202\n",
      "Time:                        10:11:57   Log-Likelihood:                -402.09\n",
      "No. Observations:                 184   AIC:                             808.2\n",
      "Df Residuals:                     182   BIC:                             814.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                        12.5972      4.162      3.027      0.003       4.386      20.809\n",
      "Final_Adjusted_Stuff_Plus    -0.1307      0.042     -3.132      0.002      -0.213      -0.048\n",
      "==============================================================================\n",
      "Omnibus:                        2.061   Durbin-Watson:                   1.865\n",
      "Prob(Omnibus):                  0.357   Jarque-Bera (JB):                1.859\n",
      "Skew:                           0.047   Prob(JB):                        0.395\n",
      "Kurtosis:                       3.484   Cond. No.                     2.60e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.6e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#x = Final_Adjusted_Stuff_Plus y = RunValue\n",
    "x = df_2023_total_pitcher_RV_count100['Final_Adjusted_Stuff_Plus']\n",
    "y = df_2023_total_pitcher_RV_count100['RunValue']\n",
    "#ols\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X).fit()\n",
    "# Print the summary\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
