{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing Basic libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model, naive_bayes\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import svm\n",
    "import bokeh as bk\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "from chart_studio.plotly import plot, iplot \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objects as go\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import pybaseball as pyb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from pybaseball import statcast\n",
    "from pybaseball import statcast_pitcher\n",
    "from pybaseball import statcast_batter\n",
    "from pybaseball import statcast_pitcher_exitvelo_barrels\n",
    "from pybaseball import statcast_batter_exitvelo_barrels\n",
    "from pybaseball import statcast_batter_expected_stats\n",
    "from pybaseball import statcast_pitcher_expected_stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pwlf as pwlf\n",
    "#import matplotlib.backends\n",
    "import matplotlib as mpl\n",
    "import joblib\n",
    "import pickle as pkl\n",
    "#from test import keras\n",
    "from keras.models import Sequential\n",
    "#dense\n",
    "from keras.layers import Dense\n",
    "#adam\n",
    "from keras.optimizers import Adam\n",
    "from datetime import datetime, timedelta\n",
    "from pybaseball import statcast\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data, Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      "  7%|▋         | 1/15 [00:13<03:15, 13.97s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 13%|█▎        | 2/15 [00:15<01:25,  6.60s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 20%|██        | 3/15 [00:15<00:43,  3.64s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 27%|██▋       | 4/15 [00:15<00:24,  2.26s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 33%|███▎      | 5/15 [00:18<00:24,  2.41s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 40%|████      | 6/15 [00:19<00:17,  2.00s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 47%|████▋     | 7/15 [00:20<00:14,  1.76s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 53%|█████▎    | 8/15 [00:21<00:10,  1.43s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 60%|██████    | 9/15 [00:23<00:09,  1.66s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 67%|██████▋   | 10/15 [00:27<00:12,  2.40s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 73%|███████▎  | 11/15 [00:27<00:06,  1.72s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 80%|████████  | 12/15 [00:28<00:03,  1.30s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 87%|████████▋ | 13/15 [00:28<00:02,  1.04s/it]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      " 93%|█████████▎| 14/15 [00:29<00:00,  1.22it/s]/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/pybaseball/datahelpers/postprocessing.py:59: FutureWarning:\n",
      "\n",
      "errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "\n",
      "100%|██████████| 15/15 [00:29<00:00,  1.96s/it]\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:29: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:30: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:84: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:86: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:87: FutureWarning:\n",
      "\n",
      "A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/895039731.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2022 = pd.read_csv(\"/Users/mikerabayda/Downloads/MLB_ALL_TIME.csv\",low_memory=False)\n",
    "df_2024 = pd.read_csv(\"/Users/mikerabayda/Downloads/Statcast_2024.csv\",low_memory=False)\n",
    "df_2022 = pd.concat([df_2022, df_2024], ignore_index=True)\n",
    "# Automatically use yesterday's date\n",
    "yesterday = datetime.today() - timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "# Pull Statcast data for yesterday only\n",
    "df_2025 = statcast(start_dt='2025-03-27', end_dt=yesterday_str)\n",
    "df_2023 = df_2025 \n",
    "\n",
    "#first four digits of the date are the year game_date\n",
    "df_2022['Year'] = df_2022['game_date'].astype(str).str[:4]\n",
    "df_2023['Year'] = df_2023['game_date'].astype(str).str[:4]\n",
    "#remove 2023 data from 2022 data\n",
    "#multiply pfx_z\tpfx_x by 12 to get inches\n",
    "df_2022['pfx_x'] = df_2022['pfx_x']*12\n",
    "df_2022['pfx_z'] = df_2022['pfx_z']*12\n",
    "df_2023['pfx_x'] = df_2023['pfx_x']*12\n",
    "df_2023['pfx_z'] = df_2023['pfx_z']*12\n",
    "\n",
    "\n",
    "#multiply pfx_x by -1 to get pitchers view\n",
    "df_2022['pfx_x'] = df_2022['pfx_x']*-1\n",
    "df_2023['pfx_x'] = df_2023['pfx_x']*-1\n",
    "\n",
    "df_2022.rename(columns={'game_date': 'Date', 'player_name':'Pitcher','p_throws':'Hand','pitch_type':'TaggedPitchType','release_speed':'RelSpeed','release_pos_x':'RelSide','release_pos_z':'RelHeight','pfx_x':'HorzBreak','pfx_z':'InducedVertBreak','release_extension':'Extension','plate_x': 'PlateLocSide','plate_z': 'PlateLocHeight', 'release_spin_rate':'SpinRate'},inplace=True)\n",
    "df_2023.rename(columns={'game_date': 'Date', 'player_name':'Pitcher','pitcher':'Pitcher_ID','p_throws':'Hand','pitch_type':'TaggedPitchType','release_speed':'RelSpeed','release_pos_x':'RelSide','release_pos_z':'RelHeight','pfx_x':'HorzBreak','pfx_z':'InducedVertBreak','release_extension':'Extension','plate_x': 'PlateLocSide','plate_z': 'PlateLocHeight','release_spin_rate':'SpinRate'},inplace=True)\n",
    "#if Event is == NaN fill with description\n",
    "df_2022['events'].fillna(df_2022['description'], inplace=True)\n",
    "df_2023['events'].fillna(df_2023['description'], inplace=True)\n",
    "\n",
    "df_2023['PitcherTeam'] = np.where(df_2023['inning_topbot'] == 'Bot', df_2023['away_team'], df_2023['home_team'])\n",
    "#df_2023 Pitcher\tTaggedPitchType\tRelSpeed\trelease_spin_rate\tInducedVertBreak\tHorzBreak\tRelSide\tRelHeight\tExtension\t\n",
    "df_2022 = df_2022[['Year','Pitcher','Hand','TaggedPitchType','events','description','RelSpeed','SpinRate','InducedVertBreak','HorzBreak','RelSide','RelHeight','Extension','PlateLocSide','PlateLocHeight','batter','launch_speed','launch_angle','hit_distance_sc','delta_run_exp']]\n",
    "df_2023 = df_2023[['Year','Pitcher','Pitcher_ID','PitcherTeam','Hand','TaggedPitchType','events','description','RelSpeed','SpinRate','InducedVertBreak','HorzBreak','RelSide','RelHeight','Extension','PlateLocSide','PlateLocHeight','batter','launch_speed','launch_angle','hit_distance_sc','delta_run_exp','bat_speed']]\n",
    "#Pitcher/Pitcher_ID / \n",
    "#run values \n",
    "field_out = -0.1955687665555\n",
    "force_out = -0.1955687665555\n",
    "other_out = -0.1955687665555\n",
    "fielders_choice_out = -0.1955687665555\n",
    "called_strike = -0.118124935770601\n",
    "swinging_strike = -0.118124935770601\n",
    "ball = 0.0636883289483747\n",
    "foul = -0.0380502742575014\n",
    "single = 0.467292970729251\n",
    "double = 0.766083122898271\n",
    "triple = 1.05755624961515\n",
    "home_run = 1.374328827219\n",
    "strikeout = -0.118124935770601\n",
    "fielders_choice = -0.1955687665555\n",
    "hit_by_pitch = 0.0636883289483747\n",
    "walk = 0.0636883289483747\n",
    "field_error = -0.1955687665555\n",
    "walk = 0.0636883289483747\n",
    "sac_fly = 0.0636883289483747\n",
    "double_play = -0.1955687665555\n",
    "wild_pitch =0.0636883289483747\n",
    "blocked_ball = 0.0636883289483747\n",
    "grounded_into_double_play = -0.1955687665555\n",
    "foul_bunt = -0.0380502742575014\n",
    "foul_tip = -0.0380502742575014\n",
    "sac_bunt_double_play = -0.1955687665555\n",
    "swinging_strike_blocked = -0.118124935770601\n",
    "missed_bunt = -0.118124935770601\n",
    "sac_bunt = 0.0636883289483747\n",
    "pitchout = 0\n",
    "caught_stealing_2b = 0\n",
    "bunt_foul_tip = -0.0380502742575014\n",
    "strikeout_double_play = -0.118124935770601\n",
    "pickoff_3b = 0 \n",
    "catcher_interf = 0\n",
    "caught_stealing_3b = 0 \n",
    "pickoff_caught_stealing_2b = 0\n",
    "triple_play = -0.118124935770601\n",
    "caught_stealing_home = 0 \n",
    "sac_fly_double_play = -0.1955687665555\n",
    "pickoff_1b = 0 \n",
    "pickoff_caught_stealing_home = 0\n",
    "pickoff_caught_stealing_3b = 0 \n",
    "game_advisory = 0\n",
    "pickoff_2b = 0\n",
    "df_2022['RunValue'] = df_2022['events'].map({'field_out':field_out,'force_out':force_out,'other_out':other_out,'fielders_choice_out':fielders_choice_out,'called_strike':called_strike,'swinging_strike':swinging_strike,'ball':ball,'foul':foul,'single':single,'double':double,'triple':triple,'home_run':home_run,'strikeout':strikeout,'fielders_choice':fielders_choice,'hit_by_pitch':hit_by_pitch,'walk':walk,'field_error':field_error,'walk':walk,'sac_fly':sac_fly,'double_play':double_play,'wild_pitch':wild_pitch,'blocked_ball':blocked_ball,'grounded_into_double_play':grounded_into_double_play,'foul_bunt':foul_bunt,'foul_tip':foul_tip,'sac_bunt_double_play':sac_bunt_double_play,'swinging_strike_blocked':swinging_strike_blocked,'missed_bunt':missed_bunt,'sac_bunt':sac_bunt,'pitchout':pitchout,'caught_stealing_2b':caught_stealing_2b,'bunt_foul_tip':bunt_foul_tip,'strikeout_double_play':strikeout_double_play,'pickoff_3b':pickoff_3b,'catcher_interf':catcher_interf,'caught_stealing_3b':caught_stealing_3b,'pickoff_caught_stealing_2b':pickoff_caught_stealing_2b,'triple_play':triple_play,'caught_stealing_home':caught_stealing_home,'sac_fly_double_play':sac_fly_double_play,'pickoff_1b':pickoff_1b,'pickoff_caught_stealing_home':pickoff_caught_stealing_home,'pickoff_caught_stealing_3b':pickoff_caught_stealing_3b,'game_advisory':game_advisory,'pickoff_2b':pickoff_2b})\n",
    "df_2023['RunValue'] = df_2023['events'].map({'field_out':field_out,'force_out':force_out,'other_out':other_out,'fielders_choice_out':fielders_choice_out,'called_strike':called_strike,'swinging_strike':swinging_strike,'ball':ball,'foul':foul,'single':single,'double':double,'triple':triple,'home_run':home_run,'strikeout':strikeout,'fielders_choice':fielders_choice,'hit_by_pitch':hit_by_pitch,'walk':walk,'field_error':field_error,'walk':walk,'sac_fly':sac_fly,'double_play':double_play,'wild_pitch':wild_pitch,'blocked_ball':blocked_ball,'grounded_into_double_play':grounded_into_double_play,'foul_bunt':foul_bunt,'foul_tip':foul_tip,'sac_bunt_double_play':sac_bunt_double_play,'swinging_strike_blocked':swinging_strike_blocked,'missed_bunt':missed_bunt,'sac_bunt':sac_bunt,'pitchout':pitchout,'caught_stealing_2b':caught_stealing_2b,'bunt_foul_tip':bunt_foul_tip,'strikeout_double_play':strikeout_double_play,'pickoff_3b':pickoff_3b,'catcher_interf':catcher_interf,'caught_stealing_3b':caught_stealing_3b,'pickoff_caught_stealing_2b':pickoff_caught_stealing_2b,'triple_play':triple_play,'caught_stealing_home':caught_stealing_home,'sac_fly_double_play':sac_fly_double_play,'pickoff_1b':pickoff_1b,'pickoff_caught_stealing_home':pickoff_caught_stealing_home,'pickoff_caught_stealing_3b':pickoff_caught_stealing_3b,'game_advisory':game_advisory,'pickoff_2b':pickoff_2b})\n",
    "#replace all Nan Events with Descriptions\n",
    "df_2022['events'].fillna(df_2022['description'], inplace=True)\n",
    "df_2023['events'].fillna(df_2023['description'], inplace=True)\n",
    "#filter only balls in play == hit_into_play\n",
    "df_2022_hit_into_play = df_2022[df_2022['description'] == 'hit_into_play']\n",
    "df_2023_hit_into_play = df_2023[df_2023['description'] == 'hit_into_play']\n",
    "df_not_hit_into_play = df_2022[df_2022['description'] != 'hit_into_play']\n",
    "df_not_hit_into_play_2023 = df_2023[df_2023['description'] != 'hit_into_play']\n",
    "#predict run value based on launch_speed','launch_angle','hit_distanc_sc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predict Run Value columns based on Launch Speed, Launch Angle, Hit Distance into df_2022, df_2023 on pitches hit into play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikerabayda/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning:\n",
      "\n",
      "[22:05:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xRun Value EV/LA Best  R^2: 0.49906635487139905383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4188320463.py:78: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4188320463.py:79: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#predict run value based on launch speed, launch angle, hit distance\n",
    "X = df_2022_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']]\n",
    "y = df_2022_hit_into_play['RunValue']\n",
    "X1 = df_2023_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']]\n",
    "y1 = df_2023_hit_into_play['RunValue']\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X1_train, X1_test, y1_train, y1_test =  train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already split into X_train, y_train:\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X1_train, y1_train, test_size=0.2, random_state=42)\n",
    "import numpy as np\n",
    "\n",
    "# Check for NaN or infinity in y1_train\n",
    "print(np.isnan(y1_train).sum(), np.isinf(y1_train).sum())\n",
    "print(np.isnan(y_val_1).sum(), np.isinf(y_val_1).sum())\n",
    "y1_train_clean = y1_train[np.isfinite(y1_train)]\n",
    "y_val_1_clean = y_val_1[np.isfinite(y_val_1)]\n",
    "X1_train_clean = X1_train[np.isfinite(y1_train)]\n",
    "X_val_1_clean = X_val_1[np.isfinite(y_val_1)]\n",
    "print(np.isnan(y1_train_clean).sum(), np.isinf(y1_train_clean).sum())\n",
    "print(np.isnan(y_val_1_clean).sum(), np.isinf(y_val_1_clean).sum())\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor(**params)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "xgb_model_1 = XGBRegressor(**params)\n",
    "xgb_model_1.fit(X1_train_clean, y1_train_clean, eval_set=[(X_val_1_clean, y_val_1_clean)], verbose=False)\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "n_estimators = 10  # Number of XGBoost models in the ensemble\n",
    "predictions = []\n",
    "\n",
    "# 1. Create bootstrap samples and train XGBoost models\n",
    "for i in range(n_estimators):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train)\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train_sample, y_train_sample, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    # 2. Predict on validation set\n",
    "    preds = model.predict(X_val)\n",
    "    predictions.append(preds)\n",
    "\n",
    "# 3. Average predictions from all models for ensemble prediction\n",
    "ensemble_preds = np.mean(predictions, axis=0)\n",
    "\n",
    "# Calculate R^2 for ensemble prediction\n",
    "ensemble_r2 = r2_score(y_val, ensemble_preds)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = xgb.cv(\n",
    "    dtrain=dtrain, \n",
    "    params=params, \n",
    "    nfold=5, \n",
    "    num_boost_round=200, \n",
    " \n",
    "    metrics=\"rmse\",\n",
    "    as_pandas=True, \n",
    "    seed=42\n",
    ")\n",
    "#Variance \n",
    "variance_y = y_train.var()\n",
    "best_r_squared = 1 - (cv_results['test-rmse-mean'].min() ** 2) / variance_y\n",
    "#print best r^2 without using scientific notation\n",
    "print('xRun Value EV/LA Best  R^2:', '{:.20f}'.format(best_r_squared))\n",
    "df_2022_hit_into_play['RunValue'] = xgb_model.predict(df_2022_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']])\n",
    "df_2023_hit_into_play['RunValue'] = xgb_model_1.predict(df_2023_hit_into_play[['launch_speed','launch_angle','hit_distance_sc']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Concatenate dfs with Run Value with balls not hit into play\n",
    "# 4. Add new features(ABS_Horizontal, ABS_RelSide, differntial_break), Split dataframes into specific pitches(FB, Sink, Slider, Sweeper, Changeup, Cutter, Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:14: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:16: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:20: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:21: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:23: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:24: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:27: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:30: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:31: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:33: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:34: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:36: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:37: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:40: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:41: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:42: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:43: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/3280302574.py:44: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#combine df_2022_hit_into_play and df_not_hit_into_play \n",
    "df_2022_1 = pd.concat([df_2022_hit_into_play,df_not_hit_into_play ],ignore_index=True)\n",
    "df_2023_1 = pd.concat([df_2023_hit_into_play,df_not_hit_into_play_2023 ],ignore_index=True)\n",
    "df_2022 = df_2022_1\n",
    "df_2023 = df_2023_1\n",
    "df_2023['TaggedPitchType'].unique()\n",
    "#remove any Nan RunValue\n",
    "df_2022 = df_2022[df_2022['RunValue'].notna()]\n",
    "df_2023 = df_2023[df_2023['RunValue'].notna()]\n",
    "\n",
    "#change pitch type to match trackman data\n",
    "#FF, SI, FS,FA  = Fastball\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['FF','FA'],'Fastball')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['FF','FA'],'Fastball')\n",
    "\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['SI'], 'Sinker')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['SI'], 'Sinker')\n",
    "\n",
    "#SL,ST,FC  = Slider\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['SL'],'Slider')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['SL'],'Slider')\n",
    "\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['ST'],'Sweeper')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['ST'],'Sweeper')\n",
    "\n",
    "#CH, EP, SC, FO = ChangeUp\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['CH','EP','SC','FO','KN'],'ChangeUp')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['CH','EP','SC','FO','KN'],'ChangeUp')\n",
    "#SV, CU, KC = Curveball\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['SV','CU','KC'],'Curveball')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['SV','CU','KC'],'Curveball')\n",
    "#'FC' = Cutter\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['FC'],'Cutter')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['FC'],'Cutter')\n",
    "#SPL = FS\n",
    "df_2022['TaggedPitchType'] = df_2022['TaggedPitchType'].replace(['FS'],'Splitter')\n",
    "df_2023['TaggedPitchType'] = df_2023['TaggedPitchType'].replace(['FS'],'Splitter')\n",
    "\n",
    "df_2022['ABS_Horizontal'] = abs(df_2022['HorzBreak'])\n",
    "df_2023['ABS_Horizontal'] = abs(df_2023['HorzBreak'])\n",
    "df_2022['ABS_RelSide'] = abs(df_2022['RelSide'])\n",
    "df_2023['ABS_RelSide'] = abs(df_2023['RelSide'])\n",
    "df_2022['differential_break'] = abs(df_2022['InducedVertBreak'] - df_2022['ABS_Horizontal'])\n",
    "df_2023['differential_break'] = abs(df_2023['InducedVertBreak'] - df_2023['ABS_Horizontal'])\n",
    "\n",
    "#for each pitcher in df_2022, df_2023, calculate differential_speed between the pitch with highest RelSpeed and any offspeed pitches they have - Offspeed\n",
    "\n",
    "\n",
    "#fastballs = 'Fastball', 'Sinker', 'TwoSeamFastBall', 'FourSeamFastBall', 'OneSeamFastBall'\n",
    "dfb2 = df_2022[df_2022.TaggedPitchType.isin(['Fastball', 'FourSeamFastball', 'OneSeamFastBall'])]\n",
    "dfb3 = df_2023[df_2023.TaggedPitchType.isin(['Fastball', 'FourSeamFastball', 'OneSeamFastBall'])]\n",
    "#'Sinker', 'TwoSeamFastBall',\n",
    "dsi2 = df_2022[df_2022.TaggedPitchType.isin(['Sinker', 'TwoSeamFastBall'])]\n",
    "dsi3 = df_2023[df_2023.TaggedPitchType.isin(['Sinker', 'TwoSeamFastBall'])]\n",
    "#sliders = 'Slider', 'Cutter'\n",
    "dsl2 = df_2022[df_2022.TaggedPitchType.isin(['Slider'])]\n",
    "dsl3 = df_2023[df_2023.TaggedPitchType.isin(['Slider'])]\n",
    "#Sweeper\n",
    "dst2 = df_2022[df_2022.TaggedPitchType.isin(['Sweeper'])]\n",
    "dst3 = df_2023[df_2023.TaggedPitchType.isin(['Sweeper'])]\n",
    "#curveballs = 'Curveball', 'KnuckleCurve'\n",
    "dcb2 = df_2022[df_2022.TaggedPitchType.isin(['Curveball', 'KnuckleCurve'])]\n",
    "dcb3 = df_2023[df_2023.TaggedPitchType.isin(['Curveball', 'KnuckleCurve'])]\n",
    "#changeups = 'Changeup', 'Splitter', 'Forkball', 'Screwball'\n",
    "dch2 = df_2022[df_2022.TaggedPitchType.isin(['ChangeUp'])]\n",
    "dch3 = df_2023[df_2023.TaggedPitchType.isin(['ChangeUp'])]\n",
    "#cutters = 'Cutter'\n",
    "dct2 = df_2022[df_2022.TaggedPitchType.isin(['Cutter'])]\n",
    "dct3 = df_2023[df_2023.TaggedPitchType.isin(['Cutter'])]\n",
    "#splitter\n",
    "dsp2 = df_2022[df_2022.TaggedPitchType.isin(['Splitter'])]\n",
    "dsp3 = df_2023[df_2023.TaggedPitchType.isin(['Splitter'])]\n",
    "#delete all rows with nan values\n",
    "dfb2 = dfb2.dropna()\n",
    "dsi2 = dsi2.dropna()\n",
    "dsl2 = dsl2.dropna()\n",
    "dst2 = dst2.dropna()\n",
    "dcb2 = dcb2.dropna()\n",
    "dch2 = dch2.dropna()\n",
    "dct2 = dct2.dropna()\n",
    "dsp2 = dsp2.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create X,y for each pitch, XGB model predicting Stuff+ for ech pitch, scale results to 100 average Stuff+ score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:162: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:163: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:164: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:165: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:166: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:167: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:168: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:169: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:225: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:226: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:227: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:233: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:234: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:235: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:241: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:242: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:243: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:249: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:250: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:251: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:257: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:258: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:259: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:265: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:266: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:267: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:273: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:274: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:275: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:281: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:282: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:283: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1674 - r_squared: -1.7709 - val_loss: 0.0600 - val_r_squared: -0.0039\n",
      "Epoch 2/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0601 - r_squared: -0.0122 - val_loss: 0.0599 - val_r_squared: -0.0036\n",
      "Epoch 3/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0591 - r_squared: -0.0058 - val_loss: 0.0599 - val_r_squared: -0.0033\n",
      "Epoch 4/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0604 - r_squared: -0.0050 - val_loss: 0.0599 - val_r_squared: -0.0030\n",
      "Epoch 5/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0597 - r_squared: -0.0053 - val_loss: 0.0599 - val_r_squared: -0.0022\n",
      "Epoch 6/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0598 - r_squared: -0.0041 - val_loss: 0.0599 - val_r_squared: -0.0032\n",
      "Epoch 7/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0601 - r_squared: -0.0032 - val_loss: 0.0599 - val_r_squared: -0.0020\n",
      "Epoch 8/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0594 - r_squared: -0.0030 - val_loss: 0.0599 - val_r_squared: -0.0036\n",
      "Epoch 9/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0596 - r_squared: -0.0030 - val_loss: 0.0599 - val_r_squared: -0.0022\n",
      "Epoch 10/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0596 - r_squared: -0.0024 - val_loss: 0.0598 - val_r_squared: -0.0016\n",
      "Epoch 11/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0601 - r_squared: -0.0019 - val_loss: 0.0599 - val_r_squared: -0.0019\n",
      "Epoch 12/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0601 - r_squared: -0.0027 - val_loss: 0.0598 - val_r_squared: -0.0019\n",
      "Epoch 13/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0593 - r_squared: -0.0019 - val_loss: 0.0598 - val_r_squared: -0.0018\n",
      "Epoch 14/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0598 - r_squared: -0.0019 - val_loss: 0.0598 - val_r_squared: -0.0018\n",
      "Epoch 15/50\n",
      "\u001b[1m1296/1296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0598 - r_squared: -0.0020 - val_loss: 0.0598 - val_r_squared: -0.0019\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1902 - r_squared: -2.7213 - val_loss: 0.0535 - val_r_squared: -0.0222\n",
      "Epoch 2/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0603 - r_squared: -0.1316 - val_loss: 0.0528 - val_r_squared: -0.0070\n",
      "Epoch 3/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0534 - r_squared: -0.0249 - val_loss: 0.0527 - val_r_squared: -0.0055\n",
      "Epoch 4/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0537 - r_squared: -0.0084 - val_loss: 0.0527 - val_r_squared: -0.0041\n",
      "Epoch 5/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0542 - r_squared: -0.0068 - val_loss: 0.0527 - val_r_squared: -0.0038\n",
      "Epoch 6/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0534 - r_squared: -0.0053 - val_loss: 0.0527 - val_r_squared: -0.0032\n",
      "Epoch 7/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0524 - r_squared: -0.0047 - val_loss: 0.0529 - val_r_squared: -0.0092\n",
      "Epoch 8/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0529 - r_squared: -0.0049 - val_loss: 0.0528 - val_r_squared: -0.0058\n",
      "Epoch 9/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0542 - r_squared: -0.0037 - val_loss: 0.0528 - val_r_squared: -0.0056\n",
      "Epoch 10/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0530 - r_squared: -0.0033 - val_loss: 0.0527 - val_r_squared: -0.0043\n",
      "Epoch 11/50\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0530 - r_squared: -0.0031 - val_loss: 0.0527 - val_r_squared: -0.0042\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1844 - r_squared: -2.2936 - val_loss: 0.0607 - val_r_squared: -0.0270\n",
      "Epoch 2/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0647 - r_squared: -0.1324 - val_loss: 0.0598 - val_r_squared: -0.0097\n",
      "Epoch 3/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0596 - r_squared: -0.0325 - val_loss: 0.0595 - val_r_squared: -0.0055\n",
      "Epoch 4/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0584 - r_squared: -0.0145 - val_loss: 0.0594 - val_r_squared: -0.0051\n",
      "Epoch 5/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0583 - r_squared: -0.0088 - val_loss: 0.0595 - val_r_squared: -0.0071\n",
      "Epoch 6/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0585 - r_squared: -0.0081 - val_loss: 0.0596 - val_r_squared: -0.0065\n",
      "Epoch 7/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0585 - r_squared: -0.0056 - val_loss: 0.0594 - val_r_squared: -0.0038\n",
      "Epoch 8/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0582 - r_squared: -0.0045 - val_loss: 0.0594 - val_r_squared: -0.0037\n",
      "Epoch 9/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0590 - r_squared: -0.0057 - val_loss: 0.0594 - val_r_squared: -0.0048\n",
      "Epoch 10/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0592 - r_squared: -0.0051 - val_loss: 0.0594 - val_r_squared: -0.0048\n",
      "Epoch 11/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0597 - r_squared: -0.0048 - val_loss: 0.0594 - val_r_squared: -0.0038\n",
      "Epoch 12/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0586 - r_squared: -0.0034 - val_loss: 0.0594 - val_r_squared: -0.0037\n",
      "Epoch 13/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0581 - r_squared: -0.0051 - val_loss: 0.0594 - val_r_squared: -0.0030\n",
      "Epoch 14/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0581 - r_squared: -0.0042 - val_loss: 0.0594 - val_r_squared: -0.0033\n",
      "Epoch 15/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0577 - r_squared: -0.0044 - val_loss: 0.0593 - val_r_squared: -0.0028\n",
      "Epoch 16/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0580 - r_squared: -0.0054 - val_loss: 0.0594 - val_r_squared: -0.0030\n",
      "Epoch 17/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0591 - r_squared: -0.0040 - val_loss: 0.0593 - val_r_squared: -0.0027\n",
      "Epoch 18/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0579 - r_squared: -0.0031 - val_loss: 0.0593 - val_r_squared: -0.0031\n",
      "Epoch 19/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0578 - r_squared: -0.0028 - val_loss: 0.0593 - val_r_squared: -0.0027\n",
      "Epoch 20/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0569 - r_squared: -0.0026 - val_loss: 0.0593 - val_r_squared: -0.0029\n",
      "Epoch 21/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0581 - r_squared: -0.0032 - val_loss: 0.0593 - val_r_squared: -0.0031\n",
      "Epoch 22/50\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0596 - r_squared: -0.0037 - val_loss: 0.0594 - val_r_squared: -0.0030\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3114 - r_squared: -4.9222 - val_loss: 0.0598 - val_r_squared: -0.0781\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1308 - r_squared: -1.5251 - val_loss: 0.0600 - val_r_squared: -0.0809\n",
      "Epoch 3/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1008 - r_squared: -0.9175 - val_loss: 0.0586 - val_r_squared: -0.0506\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0816 - r_squared: -0.5788 - val_loss: 0.0576 - val_r_squared: -0.0371\n",
      "Epoch 5/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0726 - r_squared: -0.3889 - val_loss: 0.0579 - val_r_squared: -0.0359\n",
      "Epoch 6/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0680 - r_squared: -0.2900 - val_loss: 0.0568 - val_r_squared: -0.0195\n",
      "Epoch 7/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0656 - r_squared: -0.1991 - val_loss: 0.0566 - val_r_squared: -0.0154\n",
      "Epoch 8/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0618 - r_squared: -0.1231 - val_loss: 0.0565 - val_r_squared: -0.0118\n",
      "Epoch 9/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0595 - r_squared: -0.0923 - val_loss: 0.0564 - val_r_squared: -0.0107\n",
      "Epoch 10/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0562 - r_squared: -0.0767 - val_loss: 0.0564 - val_r_squared: -0.0101\n",
      "Epoch 11/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0575 - r_squared: -0.0519 - val_loss: 0.0563 - val_r_squared: -0.0100\n",
      "Epoch 12/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0554 - r_squared: -0.0442 - val_loss: 0.0563 - val_r_squared: -0.0088\n",
      "Epoch 13/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0559 - r_squared: -0.0257 - val_loss: 0.0563 - val_r_squared: -0.0105\n",
      "Epoch 14/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0555 - r_squared: -0.0204 - val_loss: 0.0562 - val_r_squared: -0.0073\n",
      "Epoch 15/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0526 - r_squared: -0.0233 - val_loss: 0.0563 - val_r_squared: -0.0070\n",
      "Epoch 16/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0580 - r_squared: -0.0116 - val_loss: 0.0562 - val_r_squared: -0.0060\n",
      "Epoch 17/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0560 - r_squared: -0.0084 - val_loss: 0.0565 - val_r_squared: -0.0144\n",
      "Epoch 18/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0525 - r_squared: -0.0118 - val_loss: 0.0562 - val_r_squared: -0.0068\n",
      "Epoch 19/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0556 - r_squared: -0.0055 - val_loss: 0.0562 - val_r_squared: -0.0068\n",
      "Epoch 20/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0536 - r_squared: -0.0067 - val_loss: 0.0567 - val_r_squared: -0.0206\n",
      "Epoch 21/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0551 - r_squared: -0.0100 - val_loss: 0.0561 - val_r_squared: -0.0043\n",
      "Epoch 22/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0555 - r_squared: -0.0054 - val_loss: 0.0561 - val_r_squared: -0.0052\n",
      "Epoch 23/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0539 - r_squared: -0.0058 - val_loss: 0.0561 - val_r_squared: -0.0059\n",
      "Epoch 24/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0550 - r_squared: -0.0047 - val_loss: 0.0562 - val_r_squared: -0.0055\n",
      "Epoch 25/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0553 - r_squared: -0.0038 - val_loss: 0.0562 - val_r_squared: -0.0064\n",
      "Epoch 26/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0556 - r_squared: -0.0044 - val_loss: 0.0564 - val_r_squared: -0.0077\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4225 - r_squared: -6.4711 - val_loss: 0.0624 - val_r_squared: -0.1632\n",
      "Epoch 2/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1236 - r_squared: -1.3097 - val_loss: 0.0584 - val_r_squared: -0.0827\n",
      "Epoch 3/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0894 - r_squared: -0.6113 - val_loss: 0.0557 - val_r_squared: -0.0326\n",
      "Epoch 4/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0718 - r_squared: -0.3371 - val_loss: 0.0557 - val_r_squared: -0.0303\n",
      "Epoch 5/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0671 - r_squared: -0.1863 - val_loss: 0.0548 - val_r_squared: -0.0164\n",
      "Epoch 6/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0629 - r_squared: -0.1059 - val_loss: 0.0545 - val_r_squared: -0.0104\n",
      "Epoch 7/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0608 - r_squared: -0.0668 - val_loss: 0.0543 - val_r_squared: -0.0055\n",
      "Epoch 8/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0593 - r_squared: -0.0431 - val_loss: 0.0542 - val_r_squared: -0.0051\n",
      "Epoch 9/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0589 - r_squared: -0.0310 - val_loss: 0.0544 - val_r_squared: -0.0058\n",
      "Epoch 10/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0581 - r_squared: -0.0213 - val_loss: 0.0543 - val_r_squared: -0.0051\n",
      "Epoch 11/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0574 - r_squared: -0.0171 - val_loss: 0.0542 - val_r_squared: -0.0052\n",
      "Epoch 12/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0555 - r_squared: -0.0131 - val_loss: 0.0542 - val_r_squared: -0.0045\n",
      "Epoch 13/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0561 - r_squared: -0.0115 - val_loss: 0.0543 - val_r_squared: -0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0576 - r_squared: -0.0078 - val_loss: 0.0542 - val_r_squared: -0.0033\n",
      "Epoch 15/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0567 - r_squared: -0.0073 - val_loss: 0.0542 - val_r_squared: -0.0038\n",
      "Epoch 16/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0561 - r_squared: -0.0069 - val_loss: 0.0545 - val_r_squared: -0.0076\n",
      "Epoch 17/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - r_squared: -0.0080 - val_loss: 0.0543 - val_r_squared: -0.0063\n",
      "Epoch 18/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0583 - r_squared: -0.0070 - val_loss: 0.0543 - val_r_squared: -0.0055\n",
      "Epoch 19/50\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0550 - r_squared: -0.0056 - val_loss: 0.0542 - val_r_squared: -0.0034\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2118 - r_squared: -2.9701 - val_loss: 0.0581 - val_r_squared: -0.0338\n",
      "Epoch 2/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0730 - r_squared: -0.3677 - val_loss: 0.0570 - val_r_squared: -0.0134\n",
      "Epoch 3/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0594 - r_squared: -0.1051 - val_loss: 0.0565 - val_r_squared: -0.0049\n",
      "Epoch 4/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0567 - r_squared: -0.0372 - val_loss: 0.0564 - val_r_squared: -0.0037\n",
      "Epoch 5/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0562 - r_squared: -0.0173 - val_loss: 0.0564 - val_r_squared: -0.0028\n",
      "Epoch 6/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0569 - r_squared: -0.0108 - val_loss: 0.0565 - val_r_squared: -0.0071\n",
      "Epoch 7/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0547 - r_squared: -0.0089 - val_loss: 0.0564 - val_r_squared: -0.0033\n",
      "Epoch 8/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0536 - r_squared: -0.0049 - val_loss: 0.0566 - val_r_squared: -0.0065\n",
      "Epoch 9/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0547 - r_squared: -0.0061 - val_loss: 0.0564 - val_r_squared: -0.0042\n",
      "Epoch 10/50\n",
      "\u001b[1m315/315\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0552 - r_squared: -0.0066 - val_loss: 0.0564 - val_r_squared: -0.0032\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2475 - r_squared: -3.6438 - val_loss: 0.0582 - val_r_squared: -0.0553\n",
      "Epoch 2/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0839 - r_squared: -0.4758 - val_loss: 0.0567 - val_r_squared: -0.0260\n",
      "Epoch 3/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0658 - r_squared: -0.1679 - val_loss: 0.0559 - val_r_squared: -0.0117\n",
      "Epoch 4/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0587 - r_squared: -0.0726 - val_loss: 0.0557 - val_r_squared: -0.0091\n",
      "Epoch 5/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - r_squared: -0.0314 - val_loss: 0.0557 - val_r_squared: -0.0073\n",
      "Epoch 6/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0575 - r_squared: -0.0217 - val_loss: 0.0556 - val_r_squared: -0.0078\n",
      "Epoch 7/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0575 - r_squared: -0.0139 - val_loss: 0.0557 - val_r_squared: -0.0066\n",
      "Epoch 8/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0576 - r_squared: -0.0120 - val_loss: 0.0556 - val_r_squared: -0.0055\n",
      "Epoch 9/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0559 - r_squared: -0.0106 - val_loss: 0.0555 - val_r_squared: -0.0053\n",
      "Epoch 10/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0563 - r_squared: -0.0075 - val_loss: 0.0556 - val_r_squared: -0.0047\n",
      "Epoch 11/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0570 - r_squared: -0.0059 - val_loss: 0.0555 - val_r_squared: -0.0038\n",
      "Epoch 12/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0568 - r_squared: -0.0084 - val_loss: 0.0555 - val_r_squared: -0.0043\n",
      "Epoch 13/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0567 - r_squared: -0.0082 - val_loss: 0.0556 - val_r_squared: -0.0066\n",
      "Epoch 14/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0561 - r_squared: -0.0075 - val_loss: 0.0557 - val_r_squared: -0.0087\n",
      "Epoch 15/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0568 - r_squared: -0.0070 - val_loss: 0.0555 - val_r_squared: -0.0046\n",
      "Epoch 16/50\n",
      "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0560 - r_squared: -0.0069 - val_loss: 0.0556 - val_r_squared: -0.0068\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4508 - r_squared: -9.9344 - val_loss: 0.0542 - val_r_squared: -0.1430\n",
      "Epoch 2/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1708 - r_squared: -2.8736 - val_loss: 0.0527 - val_r_squared: -0.1079\n",
      "Epoch 3/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1257 - r_squared: -1.6527 - val_loss: 0.0511 - val_r_squared: -0.0669\n",
      "Epoch 4/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1035 - r_squared: -1.2311 - val_loss: 0.0522 - val_r_squared: -0.0923\n",
      "Epoch 5/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0887 - r_squared: -0.9651 - val_loss: 0.0506 - val_r_squared: -0.0548\n",
      "Epoch 6/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0781 - r_squared: -0.6962 - val_loss: 0.0512 - val_r_squared: -0.0656\n",
      "Epoch 7/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0671 - r_squared: -0.5154 - val_loss: 0.0501 - val_r_squared: -0.0429\n",
      "Epoch 8/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0678 - r_squared: -0.4105 - val_loss: 0.0501 - val_r_squared: -0.0468\n",
      "Epoch 9/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0609 - r_squared: -0.3152 - val_loss: 0.0494 - val_r_squared: -0.0283\n",
      "Epoch 10/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0566 - r_squared: -0.2909 - val_loss: 0.0490 - val_r_squared: -0.0220\n",
      "Epoch 11/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0590 - r_squared: -0.2138 - val_loss: 0.0492 - val_r_squared: -0.0241\n",
      "Epoch 12/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0536 - r_squared: -0.1740 - val_loss: 0.0488 - val_r_squared: -0.0141\n",
      "Epoch 13/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0539 - r_squared: -0.1377 - val_loss: 0.0487 - val_r_squared: -0.0142\n",
      "Epoch 14/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0504 - r_squared: -0.1006 - val_loss: 0.0485 - val_r_squared: -0.0108\n",
      "Epoch 15/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0502 - r_squared: -0.0832 - val_loss: 0.0484 - val_r_squared: -0.0088\n",
      "Epoch 16/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0508 - r_squared: -0.0780 - val_loss: 0.0485 - val_r_squared: -0.0090\n",
      "Epoch 17/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0497 - r_squared: -0.0372 - val_loss: 0.0483 - val_r_squared: -0.0056\n",
      "Epoch 18/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495 - r_squared: -0.0447 - val_loss: 0.0484 - val_r_squared: -0.0082\n",
      "Epoch 19/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492 - r_squared: -0.0382 - val_loss: 0.0483 - val_r_squared: -0.0049\n",
      "Epoch 20/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0486 - r_squared: -0.0396 - val_loss: 0.0482 - val_r_squared: -0.0044\n",
      "Epoch 21/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0495 - r_squared: -0.0273 - val_loss: 0.0481 - val_r_squared: -0.0035\n",
      "Epoch 22/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0481 - r_squared: -0.0219 - val_loss: 0.0486 - val_r_squared: -0.0154\n",
      "Epoch 23/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0471 - r_squared: -0.0582 - val_loss: 0.0482 - val_r_squared: -0.0046\n",
      "Epoch 24/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506 - r_squared: -0.0165 - val_loss: 0.0481 - val_r_squared: -0.0030\n",
      "Epoch 25/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0474 - r_squared: -0.0099 - val_loss: 0.0481 - val_r_squared: -0.0022\n",
      "Epoch 26/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0510 - r_squared: -0.0092 - val_loss: 0.0482 - val_r_squared: -0.0034\n",
      "Epoch 27/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492 - r_squared: -0.0121 - val_loss: 0.0480 - val_r_squared: -0.0015\n",
      "Epoch 28/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0465 - r_squared: -0.0045 - val_loss: 0.0480 - val_r_squared: -5.0663e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0463 - r_squared: -0.0101 - val_loss: 0.0482 - val_r_squared: -0.0044\n",
      "Epoch 30/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0478 - r_squared: -0.0051 - val_loss: 0.0480 - val_r_squared: -0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0479 - r_squared: -0.0049 - val_loss: 0.0481 - val_r_squared: -0.0020\n",
      "Epoch 32/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0481 - r_squared: -0.0038 - val_loss: 0.0480 - val_r_squared: -0.0017\n",
      "Epoch 33/50\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0483 - r_squared: -0.0083 - val_loss: 0.0480 - val_r_squared: -8.5089e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:358: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:360: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:361: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/4196405186.py:362: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#xRV stuff\n",
    "X = dfb2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y = dfb2['RunValue']\n",
    "X1 = dsi2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y1 = dsi2['RunValue']\n",
    "X2 = dsl2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y2 = dsl2['RunValue']\n",
    "X3 = dst2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y3 = dst2['RunValue']\n",
    "X4 = dcb2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y4 = dcb2['RunValue']\n",
    "X5 = dch2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y5 = dch2['RunValue']\n",
    "X6 = dct2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y6 = dct2['RunValue']\n",
    "X7 = dsp2[['RelSpeed','SpinRate','InducedVertBreak','ABS_Horizontal','RelHeight', 'ABS_RelSide', 'Extension']]\n",
    "y7 = dsp2['RunValue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.25, random_state=101)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.25, random_state=101)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.25, random_state=101)\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.25, random_state=101)\n",
    "\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y5, test_size=0.25, random_state=101)\n",
    "\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y6, test_size=0.25, random_state=101)\n",
    "\n",
    "X7_train, X7_test, y7_train, y7_test = train_test_split(X7, y7, test_size=0.25, random_state=101)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "xgb_model_0 = XGBRegressor(**params)\n",
    "xgb_model_0.fit(X_train, y_train)\n",
    "\n",
    "#sinker\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1_train, y1_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "xgb_model1 = XGBRegressor(**params)\n",
    "xgb_model1.fit(X1_train, y1_train)\n",
    "\n",
    "#cslider\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_2 = XGBRegressor(**params)\n",
    "xgb_model_2.fit(X2_train, y2_train)\n",
    "\n",
    "#csweeper\n",
    "X3_train, X3_val, y3_train, y3_val = train_test_split(X3_train, y3_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_3 = XGBRegressor(**params)\n",
    "xgb_model_3.fit(X3_train, y3_train)\n",
    "\n",
    "#curveball\n",
    "X4_train, X4_val, y4_train, y4_val = train_test_split(X4_train, y4_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_4 = XGBRegressor(**params)\n",
    "xgb_model_4.fit(X4_train, y4_train)\n",
    "\n",
    "#changeup \n",
    "X5_train, X5_val, y5_train, y5_val = train_test_split(X5_train, y5_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_5 = XGBRegressor(**params)\n",
    "xgb_model_5.fit(X5_train, y5_train)\n",
    "\n",
    "#cutter\n",
    "X6_train, X6_val, y6_train, y6_val = train_test_split(X6_train, y6_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_6 = XGBRegressor(**params)\n",
    "xgb_model_6.fit(X6_train, y6_train)\n",
    "\n",
    "#splitter\n",
    "X7_train, X7_val, y7_train, y7_val = train_test_split(X7_train, y7_train, test_size=0.2, random_state=42)\n",
    "params = {\n",
    "    'max_depth': 5, \n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'alpha': 0.05,\n",
    "    'lambda': 0.5,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "xgb_model_7 = XGBRegressor(**params)\n",
    "xgb_model_7.fit(X7_train, y7_train)\n",
    "dfb2['xRV_xgb']= xgb_model_0.predict(dfb2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsi2['xRV_xgb']= xgb_model1.predict(dsi2[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsl2['xRV_xgb']= xgb_model_2.predict(dsl2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dst2['xRV_xgb']= xgb_model_3.predict(dst2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dcb2['xRV_xgb']= xgb_model_4.predict(dcb2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dch2['xRV_xgb']= xgb_model_5.predict(dch2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dct2['xRV_xgb']= xgb_model_6.predict(dct2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsp2['xRV_xgb']= xgb_model_7.predict(dsp2[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dfb3['xRV_xgb']= xgb_model_0.predict(dfb3[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsi3['xRV_xgb']= xgb_model1.predict(dsi3[['RelSpeed','SpinRate','differential_break','RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsl3['xRV_xgb']= xgb_model_2.predict(dsl3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dst3['xRV_xgb']= xgb_model_3.predict(dst3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dcb3['xRV_xgb']= xgb_model_4.predict(dcb3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dch3['xRV_xgb']= xgb_model_5.predict(dch3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dct3['xRV_xgb']= xgb_model_6.predict(dct3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "dsp3['xRV_xgb']= xgb_model_7.predict(dsp3[['RelSpeed', 'SpinRate','InducedVertBreak', 'ABS_Horizontal', 'RelHeight', 'ABS_RelSide', 'Extension']])\n",
    "#find the r^2 for all the variables\n",
    "#create the FB expected whiff rate\n",
    "X = dfb2[['xRV_xgb']]\n",
    "y = dfb2['RunValue']\n",
    "X2 = dsi2[['xRV_xgb']]\n",
    "y2 = dsi2['RunValue']\n",
    "X3 = dsl2[['xRV_xgb']]\n",
    "y3 = dsl2['RunValue']\n",
    "X4 = dst2[['xRV_xgb']]\n",
    "y4 = dst2['RunValue']\n",
    "X5 = dcb2[['xRV_xgb']]\n",
    "y5 = dcb2['RunValue']\n",
    "X6 = dch2[['xRV_xgb']]\n",
    "y6 = dch2['RunValue']\n",
    "X7 = dct2[['xRV_xgb']]\n",
    "y7 = dct2['RunValue']\n",
    "X8 = dsp2[['xRV_xgb']]\n",
    "y8 = dsp2['RunValue']\n",
    "dfb2_max = dfb2['xRV_xgb'].max()\n",
    "dsi2_max = dsi2['xRV_xgb'].max()\n",
    "dsl2_max = dsl2['xRV_xgb'].max()\n",
    "dst2_max = dst2['xRV_xgb'].max()\n",
    "dcb2_max = dcb2['xRV_xgb'].max()\n",
    "dch2_max = dch2['xRV_xgb'].max()\n",
    "dct2_max = dct2['xRV_xgb'].max()\n",
    "dsp2_max = dsp2['xRV_xgb'].max()\n",
    "dfb3_max = dfb3['xRV_xgb'].max()\n",
    "dsi3_max = dsi3['xRV_xgb'].max()\n",
    "dsl3_max = dsl3['xRV_xgb'].max()\n",
    "dst3_max = dst3['xRV_xgb'].max()\n",
    "dcb3_max = dcb3['xRV_xgb'].max()\n",
    "dch3_max = dch3['xRV_xgb'].max()\n",
    "dct3_max = dct3['xRV_xgb'].max()\n",
    "dsp3_max = dsp3['xRV_xgb'].max()\n",
    "dfb2_max = dfb2['xRV_xgb'].max()\n",
    "dsi2_max = dsi2['xRV_xgb'].max()\n",
    "dsl2_max = dsl2['xRV_xgb'].max()\n",
    "dst2_max = dst2['xRV_xgb'].max()\n",
    "dcb2_max = dcb2['xRV_xgb'].max()\n",
    "dch2_max = dch2['xRV_xgb'].max()\n",
    "dct2_max = dct2['xRV_xgb'].max()\n",
    "dsp2_max = dsp2['xRV_xgb'].max()\n",
    "dfb3_max = dfb3['xRV_xgb'].max()\n",
    "dsi3_max = dsi3['xRV_xgb'].max()\n",
    "dsl3_max = dsl3['xRV_xgb'].max()\n",
    "dst3_max = dst3['xRV_xgb'].max()\n",
    "dcb3_max = dcb3['xRV_xgb'].max()\n",
    "dch3_max = dch3['xRV_xgb'].max()\n",
    "dct3_max = dct3['xRV_xgb'].max()\n",
    "dsp3_max = dsp3['xRV_xgb'].max()\n",
    "\n",
    "#scaled \n",
    "dfb2['xRV_Scaled'] = dfb2['xRV_xgb'] - dfb2_max\n",
    "dfb2['xRV/100_stuff_scaled_abs'] = abs(dfb2['xRV_Scaled'])\n",
    "dfb2['Stuff_plus'] = dfb2['xRV/100_stuff_scaled_abs'] / dfb2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dfb3['xRV_Scaled'] = dfb3['xRV_xgb'] - dfb3_max\n",
    "dfb3['xRV/100_stuff_scaled_abs'] = abs(dfb3['xRV_Scaled'])\n",
    "dfb3['Stuff_plus'] = dfb3['xRV/100_stuff_scaled_abs'] / dfb3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dsi2['xRV_Scaled'] = dsi2['xRV_xgb'] - dsi2_max\n",
    "dsi2['xRV/100_stuff_scaled_abs'] = abs(dsi2['xRV_Scaled'])\n",
    "dsi2['Stuff_plus'] = dsi2['xRV/100_stuff_scaled_abs'] / dsi2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dsi3['xRV_Scaled'] = dsi3['xRV_xgb'] - dsi3_max\n",
    "dsi3['xRV/100_stuff_scaled_abs'] = abs(dsi3['xRV_Scaled'])\n",
    "dsi3['Stuff_plus'] = dsi3['xRV/100_stuff_scaled_abs'] / dsi3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dsl2['xRV_Scaled'] = dsl2['xRV_xgb'] - dfb2_max\n",
    "dsl2['xRV/100_stuff_scaled_abs'] = abs(dsl2['xRV_Scaled'])\n",
    "dsl2['Stuff_plus'] = dsl2['xRV/100_stuff_scaled_abs'] / dsl2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dsl3['xRV_Scaled'] = dsl3['xRV_xgb'] - dsl3_max\n",
    "dsl3['xRV/100_stuff_scaled_abs'] = abs(dsl3['xRV_Scaled'])\n",
    "dsl3['Stuff_plus'] = dsl3['xRV/100_stuff_scaled_abs'] / dsl3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dst2['xRV_Scaled'] = dst2['xRV_xgb'] - dst2_max\n",
    "dst2['xRV/100_stuff_scaled_abs'] = abs(dst2['xRV_Scaled'])\n",
    "dst2['Stuff_plus'] = dst2['xRV/100_stuff_scaled_abs'] / dst2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dst3['xRV_Scaled'] = dst3['xRV_xgb'] - dst3_max\n",
    "dst3['xRV/100_stuff_scaled_abs'] = abs(dst3['xRV_Scaled'])\n",
    "dst3['Stuff_plus'] = dst3['xRV/100_stuff_scaled_abs'] / dst3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dcb2['xRV_Scaled'] = dcb2['xRV_xgb'] - dcb2_max\n",
    "dcb2['xRV/100_stuff_scaled_abs'] = abs(dcb2['xRV_Scaled'])\n",
    "dcb2['Stuff_plus'] = dcb2['xRV/100_stuff_scaled_abs'] / dcb2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dcb3['xRV_Scaled'] = dcb3['xRV_xgb'] - dcb3_max\n",
    "dcb3['xRV/100_stuff_scaled_abs'] = abs(dcb3['xRV_Scaled'])\n",
    "dcb3['Stuff_plus'] = dcb3['xRV/100_stuff_scaled_abs'] / dcb3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dch2['xRV_Scaled'] = dch2['xRV_xgb'] - dch2_max\n",
    "dch2['xRV/100_stuff_scaled_abs'] = abs(dch2['xRV_Scaled'])\n",
    "dch2['Stuff_plus'] = dch2['xRV/100_stuff_scaled_abs'] / dch2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dch3['xRV_Scaled'] = dch3['xRV_xgb'] - dch3_max\n",
    "dch3['xRV/100_stuff_scaled_abs'] = abs(dch3['xRV_Scaled'])\n",
    "dch3['Stuff_plus'] = dch3['xRV/100_stuff_scaled_abs'] / dch3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#scaled \n",
    "dct2['xRV_Scaled'] = dct2['xRV_xgb'] - dct2_max\n",
    "dct2['xRV/100_stuff_scaled_abs'] = abs(dct2['xRV_Scaled'])\n",
    "dct2['Stuff_plus'] = dct2['xRV/100_stuff_scaled_abs'] / dct2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dct3['xRV_Scaled'] = dct3['xRV_xgb'] - dct3_max\n",
    "dct3['xRV/100_stuff_scaled_abs'] = abs(dct3['xRV_Scaled'])\n",
    "dct3['Stuff_plus'] = dct3['xRV/100_stuff_scaled_abs'] / dct3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "#splitter\n",
    "dsp2['xRV_Scaled'] = dsp2['xRV_xgb'] - dsp2_max\n",
    "dsp2['xRV/100_stuff_scaled_abs'] = abs(dsp2['xRV_Scaled'])\n",
    "dsp2['Stuff_plus'] = dsp2['xRV/100_stuff_scaled_abs'] / dsp2['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "dsp3['xRV_Scaled'] = dsp3['xRV_xgb'] - dsp3_max\n",
    "dsp3['xRV/100_stuff_scaled_abs'] = abs(dsp3['xRV_Scaled'])\n",
    "dsp3['Stuff_plus'] = dsp3['xRV/100_stuff_scaled_abs'] / dsp3['xRV/100_stuff_scaled_abs'].mean() * 100\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, ReLU, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Custom R^2 metric\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())\n",
    "\n",
    "def build_model(input_dim):\n",
    "    inp = Input(shape=(input_dim,))\n",
    "    x = Dense(128)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    res1 = x\n",
    "\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Add()([x, res1])\n",
    "\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    out = Dense(1)(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=Adam(1e-3), loss='mse', metrics=[r_squared])\n",
    "    return model\n",
    "\n",
    "# For each pitch type\n",
    "data_dict = {\n",
    "    'fb': (dfb2, dfb3, X_train, X_test, y_train, y_test),\n",
    "    'si': (dsi2, dsi3, X1_train, X1_test, y1_train, y1_test),\n",
    "    'sl': (dsl2, dsl3, X2_train, X2_test, y2_train, y2_test),\n",
    "    'st': (dst2, dst3, X3_train, X3_test, y3_train, y3_test),\n",
    "    'cb': (dcb2, dcb3, X4_train, X4_test, y4_train, y4_test),\n",
    "    'ch': (dch2, dch3, X5_train, X5_test, y5_train, y5_test),\n",
    "    'ct': (dct2, dct3, X6_train, X6_test, y6_train, y6_test),\n",
    "    'sp': (dsp2, dsp3, X7_train, X7_test, y7_train, y7_test)\n",
    "}\n",
    "\n",
    "scalers = {}\n",
    "models = {}\n",
    "\n",
    "for label, (df_train, df_eval, X_tr, X_te, y_tr, y_te) in data_dict.items():\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "    X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "    model = build_model(X_tr_scaled.shape[1])\n",
    "    es = EarlyStopping(monitor='val_r_squared', mode='max', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_tr_scaled, y_tr, validation_data=(X_te_scaled, y_te),\n",
    "              epochs=50, batch_size=256, callbacks=[es], verbose=1)\n",
    "\n",
    "    # Store models and scalers\n",
    "    scalers[label] = scaler\n",
    "    models[label] = model\n",
    "\n",
    "    # Predict and assign\n",
    "    df_train_scaled = scaler.transform(df_train[X_tr.columns])\n",
    "    df_eval_scaled = scaler.transform(df_eval[X_tr.columns])\n",
    "\n",
    "    df_train['xRV_nn'] = model.predict(df_train_scaled, verbose=0)\n",
    "    df_eval['xRV_nn'] = model.predict(df_eval_scaled, verbose=0)\n",
    "    # === Scale xRV_nn to Stuff_plus_NN (mirrors XGB scaling logic) ===\n",
    "    df_eval['xRV_Scaled_NN'] = df_eval['xRV_nn'] - df_eval['xRV_nn'].max()\n",
    "    df_eval['xRV/100_stuff_scaled_abs_NN'] = abs(df_eval['xRV_Scaled_NN'])\n",
    "    df_eval['Stuff_plus_NN'] = df_eval['xRV/100_stuff_scaled_abs_NN'] / df_eval['xRV/100_stuff_scaled_abs_NN'].mean() * 100\n",
    "    df_train['xRV_Scaled_NN'] = df_train['xRV_nn'] - df_train['xRV_nn'].max()\n",
    "    df_train['xRV/100_stuff_scaled_abs_NN'] = abs(df_train['xRV_Scaled_NN'])\n",
    "    df_train['Stuff_plus_NN'] = df_train['xRV/100_stuff_scaled_abs_NN'] / df_train['xRV/100_stuff_scaled_abs_NN'].mean() * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Scaling to ensure Stuff+ ~ N(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:11: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:26: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:41: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:56: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:71: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:86: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:101: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:117: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB STD after scaling: 10.0\n",
      "FB MEAN after scaling: 100.00002\n",
      "SI STD after scaling: 9.999999\n",
      "SI MEAN after scaling: 100.00001\n",
      "SL STD after scaling: 9.999999\n",
      "SL MEAN after scaling: 99.99999\n",
      "ST STD after scaling: 10.0\n",
      "ST MEAN after scaling: 100.00001\n",
      "CB STD after scaling: 10.0\n",
      "CB MEAN after scaling: 100.0\n",
      "CH STD after scaling: 9.999999\n",
      "CH MEAN after scaling: 100.0\n",
      "CT STD after scaling: 10.0\n",
      "CT MEAN after scaling: 100.0\n",
      "SP STD after scaling: 9.999999\n",
      "SP MEAN after scaling: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/14gmp9zs33512s20nwz_329c0000gn/T/ipykernel_35379/1630595675.py:129: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming df_total is your DataFrame and 'Stuff_plus' is the name of your column\n",
    "current_std_fb = dfb3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_fb\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dfb3['Stuff_plus'].mean()\n",
    "dfb3['Final_Adjusted_Stuff_Plus'] = ((dfb3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dfb3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dfb3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_si = dsi3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_si\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dsi3['Stuff_plus'].mean()\n",
    "dsi3['Final_Adjusted_Stuff_Plus'] = ((dsi3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dsi3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dsi3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_sl = dsl3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_sl\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dsl3['Stuff_plus'].mean()\n",
    "dsl3['Final_Adjusted_Stuff_Plus'] = ((dsl3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dsl3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dsl3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_st = dst3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_st\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dst3['Stuff_plus'].mean()\n",
    "dst3['Final_Adjusted_Stuff_Plus'] = ((dst3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dst3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dst3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_cb = dcb3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_cb\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dcb3['Stuff_plus'].mean()\n",
    "dcb3['Final_Adjusted_Stuff_Plus'] = ((dcb3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dcb3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dcb3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_ch = dch3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_ch\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dch3['Stuff_plus'].mean()\n",
    "dch3['Final_Adjusted_Stuff_Plus'] = ((dch3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dch3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dch3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "current_std_ct = dct3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_ch\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dct3['Stuff_plus'].mean()\n",
    "dct3['Final_Adjusted_Stuff_Plus'] = ((dct3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Verify the transformation by calculating the new standard deviation and mean\n",
    "new_std = dct3['Final_Adjusted_Stuff_Plus'].std()\n",
    "new_mean = dct3['Final_Adjusted_Stuff_Plus'].mean()\n",
    "\n",
    "#splitter\n",
    "current_std_sp = dsp3['Stuff_plus'].std()\n",
    "\n",
    "# Determine the scaling factor needed to adjust the standard deviation to 10\n",
    "desired_std = 10\n",
    "scaling_factor = desired_std / current_std_sp\n",
    "\n",
    "# Apply the scaling factor to each value in the column\n",
    "# Subtract the mean, scale the zero-mean data, and then add the original mean back\n",
    "mean = dsp3['Stuff_plus'].mean()\n",
    "dsp3['Final_Adjusted_Stuff_Plus'] = ((dsp3['Stuff_plus'] - mean) * scaling_factor) + mean\n",
    "\n",
    "#combine all the dataframes\n",
    "df_2022 = pd.concat([dfb2, dsi2, dsl2, dst2, dcb2, dch2, dct2, dsp2], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# === Scale Stuff_plus from xRV_nn ===\n",
    "for label, (_, df_eval, _, _, _, _) in data_dict.items():\n",
    "    std = df_eval['Stuff_plus_NN'].std()\n",
    "    mean = df_eval['Stuff_plus_NN'].mean()\n",
    "    scaling_factor = 10 / std\n",
    "    df_eval['Final_Adjusted_Stuff_Plus_NN'] = ((df_eval['Stuff_plus_NN'] - mean) * scaling_factor) + mean\n",
    "\n",
    "# Optionally, verify the transformation\n",
    "    print(f\"{label.upper()} STD after scaling:\", df_eval['Final_Adjusted_Stuff_Plus_NN'].std())\n",
    "    print(f\"{label.upper()} MEAN after scaling:\", df_eval['Final_Adjusted_Stuff_Plus_NN'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb3_grouped = dfb3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',  # or 'last', depending on which one you want\n",
    "    'Pitcher_ID': 'first',  # or 'last', depending on which one you want\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'differential_break': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dsi3_grouped = dsi3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'differential_break': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dsl3_grouped = dsl3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dst3_grouped = dst3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dcb3_grouped = dcb3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dch3_grouped = dch3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "dct3_grouped = dct3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "    \n",
    "\n",
    "dsp3_grouped = dsp3.groupby('Pitcher').agg({\n",
    "    'PitcherTeam': 'first',\n",
    "    'Pitcher_ID': 'first',\n",
    "    'RelSpeed': 'mean',\n",
    "    'SpinRate': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'ABS_Horizontal': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'RunValue': 'mean',\n",
    "    'bat_speed': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean'\n",
    "})\n",
    "\n",
    "# === Grouping with both XGB and NN outputs ===\n",
    "grouped_data = {}\n",
    "\n",
    "for label, (_, df_eval, _, _, _, _) in data_dict.items():\n",
    "    common_features = {\n",
    "        'PitcherTeam': 'first',\n",
    "        'Pitcher_ID': 'first',\n",
    "        'RelSpeed': 'mean',\n",
    "        'SpinRate': 'mean',\n",
    "        'RelHeight': 'mean',\n",
    "        'ABS_RelSide': 'mean',\n",
    "        'Extension': 'mean',\n",
    "        'RunValue': 'mean',\n",
    "        'bat_speed': 'mean',\n",
    "        'Final_Adjusted_Stuff_Plus': 'mean',\n",
    "        'Final_Adjusted_Stuff_Plus_NN': 'mean'\n",
    "    }\n",
    "    if 'InducedVertBreak' in df_eval.columns:\n",
    "        common_features['InducedVertBreak'] = 'mean'\n",
    "    if 'ABS_Horizontal' in df_eval.columns:\n",
    "        common_features['ABS_Horizontal'] = 'mean'\n",
    "\n",
    "    grouped_data[label] = df_eval.groupby('Pitcher').agg(common_features).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_total = pd.concat([dfb3, dsi3, dsl3, dst3, dcb3, dch3, dct3, dsp3], ignore_index=True)\n",
    "#groupby pitcher and TaggedPitchType\n",
    "df_2023_total_pitch_RV = df_2023_total.groupby(['Pitcher','Pitcher_ID','PitcherTeam','TaggedPitchType']).agg({'RelSpeed': 'mean','InducedVertBreak':'mean','HorzBreak':'mean','RelHeight':'mean','ABS_RelSide': 'mean', 'Extension':'mean','Final_Adjusted_Stuff_Plus': 'mean', 'RunValue': 'sum', 'launch_speed':'mean', 'bat_speed':'mean'}).reset_index()\n",
    "df_2023_total_pitcher_RV = df_2023_total.groupby(['Pitcher','Pitcher_ID','PitcherTeam']).agg({'Final_Adjusted_Stuff_Plus': 'mean', 'RunValue': 'sum', 'launch_speed':'mean','bat_speed':'mean'}).reset_index()\n",
    "df_2023_total_pitcher_RV_release = df_2023_total.groupby(['Pitcher']).agg({'RelHeight':'mean','ABS_RelSide': 'mean', 'Extension':'mean'}).reset_index()\n",
    "df_2023_total_pitcher_RV['Count'] = df_2023_total.groupby(['Pitcher']).size().reset_index(name='Count')['Count']\n",
    "top_25_all = df_2023_total_pitcher_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "#top_25_count_50  over 50 pitches\n",
    "top_25_count_50 = df_2023_total_pitcher_RV[df_2023_total_pitcher_RV['Count'] > 50].sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "top_pitches = df_2023_total_pitch_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "\n",
    "\n",
    "\n",
    "# === Full concat and final groupings ===\n",
    "df_2023_total = pd.concat([dfb3, dsi3, dsl3, dst3, dcb3, dch3, dct3, dsp3], ignore_index=True)\n",
    "\n",
    "# Grouped by pitch type\n",
    "agg_cols = {\n",
    "    'RelSpeed': 'mean',\n",
    "    'InducedVertBreak': 'mean',\n",
    "    'HorzBreak': 'mean',\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus_NN': 'mean',\n",
    "    'RunValue': 'sum',\n",
    "    'launch_speed': 'mean',\n",
    "    'bat_speed': 'mean'\n",
    "}\n",
    "df_2023_total_pitch_RV = df_2023_total.groupby(\n",
    "    ['Pitcher', 'Pitcher_ID', 'PitcherTeam', 'TaggedPitchType']\n",
    ").agg(agg_cols).reset_index()\n",
    "\n",
    "# Grouped by pitcher\n",
    "df_2023_total_pitcher_RV = df_2023_total.groupby(\n",
    "    ['Pitcher', 'Pitcher_ID', 'PitcherTeam']\n",
    ").agg({\n",
    "    'Final_Adjusted_Stuff_Plus': 'mean',\n",
    "    'Final_Adjusted_Stuff_Plus_NN': 'mean',\n",
    "    'RunValue': 'sum',\n",
    "    'launch_speed': 'mean',\n",
    "    'bat_speed': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_2023_total_pitcher_RV_release = df_2023_total.groupby(\n",
    "    ['Pitcher']\n",
    ").agg({\n",
    "    'RelHeight': 'mean',\n",
    "    'ABS_RelSide': 'mean',\n",
    "    'Extension': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Add pitch count\n",
    "df_2023_total_pitcher_RV['Count'] = df_2023_total.groupby(\n",
    "    ['Pitcher']\n",
    ").size().reset_index(name='Count')['Count']\n",
    "\n",
    "# Top lists\n",
    "top_25_all = df_2023_total_pitcher_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "top_25_count_50 = df_2023_total_pitcher_RV[\n",
    "    df_2023_total_pitcher_RV['Count'] > 50\n",
    "].sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "top_pitches = df_2023_total_pitch_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False).head(25)\n",
    "\n",
    "top_25_all_nn = df_2023_total_pitcher_RV.sort_values(by='Final_Adjusted_Stuff_Plus_NN', ascending=False).head(25)\n",
    "top_25_count_50_nn = df_2023_total_pitcher_RV[\n",
    "    df_2023_total_pitcher_RV['Count'] > 50\n",
    "].sort_values(by='Final_Adjusted_Stuff_Plus_NN', ascending=False).head(25)\n",
    "top_pitches_nn = df_2023_total_pitch_RV.sort_values(by='Final_Adjusted_Stuff_Plus_NN', ascending=False).head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Pitcher IDs: <IntegerArray>\n",
      "[605452, 621383, 660853, 672578, 666200, 680885, 685801, 669438, 641793,\n",
      " 676282,\n",
      " ...\n",
      " 702352, 445926, 664208, 445276, 669203, 687765, 677649, 668933, 681799,\n",
      " 689254]\n",
      "Length: 433, dtype: Int64\n",
      "Pitcher Mapping: {605452: 'Ross, Joe', 621383: 'Banks, Tanner', 660853: 'De Los Santos, Enyel', 672578: 'Hernández, Carlos', 666200: 'Luzardo, Jesús', 680885: 'Schwellenbach, Spencer', 685801: 'Bigge, Hunter', 669438: 'Englert, Mason', 641793: 'Littell, Zack', 676282: 'Cantillo, Joey', 605182: 'Clevinger, Mike', 596001: 'Junis, Jakob', 686563: 'Cannon, Jonathan', 668909: 'Williams, Gavin', 680704: 'Sandlin, Nick', 547973: 'Chapman, Aroldis', 554340: 'García, Yimi', 643338: 'Green, Chad', 458677: 'Wilson, Justin', 621111: 'Buehler, Walker', 605135: 'Bassitt, Chris', 608032: 'Estévez, Carlos', 686973: 'Varland, Louis', 668674: 'Erceg, Lucas', 663485: 'Sands, Cole', 641927: 'Ober, Bailey', 608379: 'Wacha, Michael', 606303: 'Payamps, Joel', 663542: 'Hudson, Bryan', 663372: 'Feltner, Ryan', 682990: 'Priester, Quinn', 664126: 'Fairbanks, Pete', 682254: 'Montgomery, Mason', 672282: 'Detmers, Reid', 579328: 'Kikuchi, Yusei', 686752: 'Pepiot, Ryan', 669276: 'Lee, Dylan', 572955: 'Johnson, Pierce', 614179: 'Ruiz, José', 592836: 'Walker, Taijuan', 656550: 'Holmes, Grant', 681911: 'Vesia, Alex', 605347: 'López, Jorge', 674285: 'Salazar, Eduardo', 489446: 'Yates, Kirby', 607455: 'Banda, Anthony', 663623: 'Irvin, Jake', 676263: 'Dreyer, Jack', 689017: 'Knack, Landon', 682120: 'Herrin, Tim', 682171: 'Murfee, Penn', 671106: 'Allen, Logan', 680732: 'Burke, Sean', 669199: 'Bachar, Lake', 621242: 'Díaz, Edwin', 623211: 'Brazobán, Huascar', 676974: 'Meyer, Max', 592773: 'Stanek, Ryne', 668820: 'Kranick, Max', 656731: 'Megill, Tylor', 664854: 'Helsley, Ryan', 642701: 'Santana, Dennis', 681517: 'Leahy, Kyle', 657571: 'Ferguson, Caleb', 656605: 'Keller, Mitch', 661395: 'Duran, Jhoan', 643377: 'Jax, Griffin', 608717: 'Stratton, Chris', 657746: 'Ryan, Joe', 669674: 'Long, Sam', 607625: 'Lugo, Seth', 642207: 'Williams, Devin', 605154: 'Brebbia, John', 596133: 'Weaver, Luke', 656427: 'Flaherty, Jack', 608331: 'Fried, Max', 670174: 'Winckowski, Josh', 669711: 'Weissert, Greg', 656546: 'Hoffman, Jeff', 592332: 'Gausman, Kevin', 676395: 'Garcia, Robert', 669212: 'Morgan, Eli', 663878: 'Pearson, Nate', 641816: 'Mahle, Tyler', 684007: 'Imanaga, Shota', 650556: 'Abreu, Bryan', 595345: 'Okert, Steven', 664351: 'Contreras, Luis', 686613: 'Brown, Hunter', 676092: 'Snider, Collin', 660825: 'Bazardo, Eduard', 622379: 'Castillo, Luis F.', 663158: 'Suarez, Robert', 669093: 'Estrada, Jeremiah', 622780: 'Perdomo, Angel', 674370: 'Bido, Osvaldo', 686826: 'Jarvis, Bryce', 681882: 'Selby, Colin', 656222: 'Beeks, Jalen', 672335: 'Pérez, Cionel', 694297: 'Pfaadt, Brandon', 665152: 'Kremer, Dean', 680767: 'Vodnik, Victor', 641755: 'Kinley, Tyler', 683409: 'Chivilli, Angel', 683232: 'Mears, Nick', 678821: 'Peralta, Luis', 622608: 'Senzatela, Antonio', 641302: 'Alexander, Tyler', 641941: 'Pagán, Emilio', 669062: 'Miller, Erik', 664139: 'Gibaut, Ian', 663574: 'Santillan, Tony', 678495: 'Rodríguez, Randy', 607259: 'Martinez, Nick', 434378: 'Verlander, Justin', 656271: 'Burke, Brock', 670955: 'Uceta, Edwin', 690829: 'Joyce, Ben', 669358: 'Baz, Shane', 543294: 'Hendricks, Kyle', 676508: 'Casparius, Ben', 687377: 'Ribalta, Orlando', 680736: 'Wrobleski, Justin', 695418: 'Lord, Brad', 668941: 'Romero, JoJo', 694973: 'Skenes, Paul', 543243: 'Gray, Sonny', 678215: 'Arias, Luarbert', 675540: 'Curry, Xzavion', 687362: 'Gillispie, Connor', 605280: 'Holmes, Clay', 628452: 'Iglesias, Raisel', 689147: 'Kerkering, Orion', 554430: 'Wheeler, Zack', 519242: 'Sale, Chris', 606930: 'Barnes, Jacob', 684320: 'Rodríguez, Yariel', 676979: 'Crochet, Garrett', 687922: 'Lucas, Easton', 669373: 'Skubal, Tarik', 471911: 'Carrasco, Carlos', 656457: 'Gilbert, Tyler', 623149: 'Sewald, Paul', 681343: 'Smith, Shane', 671922: 'Smith, Cade', 594902: 'Lively, Ben', 519151: 'Pressly, Ryan', 455119: 'Martin, Chris', 687863: 'Hodge, Porter', 657240: 'Merryweather, Julian', 687847: 'Church, Marc', 592791: 'Taillon, Jameson', 657097: 'Webb, Jacob', 670167: 'Schreiber, John', 672582: 'Zerpa, Angel', 666142: 'Ragans, Cole', 641154: 'López, Pablo', 687473: 'Gusto, Ryan', 662253: 'Muñoz, Andrés', 687911: 'King, Bryan', 622491: 'Castillo, Luis', 806185: 'Birdsong, Hayden', 666157: 'Lodolo, Nick', 640462: 'Puk, A.J.', 669211: 'Akin, Keegan', 571946: 'Miller, Shelby', 518876: 'Kelly, Merrill', 450203: 'Morton, Charlie', 621053: 'Ferguson, Tyler', 686993: 'Sterner, Justin', 673513: 'Matsui, Yuki', 600917: 'Leclerc, José', 689690: 'Jacob, Alek', 605488: 'Springs, Jeffrey', 656302: 'Cease, Dylan', 623474: 'Herget, Jimmy', 642547: 'Peralta, Freddy', 607536: 'Freeland, Kyle', 621244: 'Berríos, José', 690916: 'Fitts, Richard', 689225: 'Brieske, Beau', 607074: 'Rodón, Carlos', 663554: 'Mize, Casey', 640448: 'Finnegan, Kyle', 608371: 'Sims, Lucas', 669022: 'Gore, MacKenzie', 669160: 'May, Dustin', 666277: 'Soriano, George', 685107: 'Veneziano, Anthony', 673540: 'Senga, Kodai', 678368: 'Bellozo, Valente', 802419: 'Harrington, Thomas', 688297: 'Roycroft, Chris', 669461: 'Liberatore, Matthew', 669387: 'Mlodzinski, Carmen', 592094: 'Adam, Jason', 650633: 'King, Michael', 622663: 'Severino, Luis', 669194: 'Nelson, Ryne', 621107: 'Eflin, Zach', 668678: 'Gallen, Zac', 656240: 'Blewett, Scott', 640451: 'Harvey, Hunter', 680573: 'Woods Richardson, Simeon', 547179: 'Lorenzen, Michael', 607067: 'Rea, Colin', 657006: 'Steele, Justin', 642520: 'Garabito, Gerson', 543135: 'Eovaldi, Nathan', 668881: 'Greene, Hunter', 642152: 'Trivino, Lou', 657277: 'Webb, Logan', 605463: 'Scott, Tayler', 669713: 'Wesneski, Hayden', 669302: 'Gilbert, Logan', 681676: 'Fernandez, Ryan', 686580: 'Slaten, Justin', 669467: 'Pallante, Andre', 656794: 'Newcomb, Sean', 621345: 'Minter, A.J.', 670102: 'Francis, Bowden', 656849: 'Peterson, David', 605447: 'Romano, Jordan', 673929: 'Leasure, Jordan', 527048: 'Pérez, Martín', 695549: 'Jobe, Jackson', 700669: 'Graceffo, Gordon', 690928: 'Dobbins, Hunter', 571945: 'Mikolas, Miles', 676962: 'Brown, Ben', 670810: 'Gillaspie, Logan', 592866: 'Williams, Trevor', 666214: 'Wentz, Joey', 687396: 'Headrick, Brent', 571760: 'Heaney, Andrew', 701542: 'Warren, Will', 693433: 'Woo, Bryan', 682847: 'Ortiz, Luis L.', 542881: 'Anderson, Tyler', 801403: 'Dollander, Chase', 683155: 'Estes, Joey', 663460: 'Bubic, Kris', 700249: 'Povich, Cade', 608718: 'Suter, Brent', 694477: 'Patrick, Chad', 594580: 'Moll, Sam', 686730: 'Spiers, Carson', 655889: 'Rodríguez, Manuel', 571948: 'Milner, Hoby', 656876: 'Rasmussen, Drew', 677958: 'Rocker, Kumar', 606160: 'Montero, Rafael', 663978: 'Paddack, Chris', 669854: 'Blanco, Ronel', 680730: 'Parker, Mitchell', 593958: 'Rodriguez, Eduardo', 660761: 'Suarez, José', 700363: 'Smith-Shawver, AJ', 615698: 'Quantrill, Cal', 573204: 'Thielbar, Caleb', 678316: 'Cruz, Omar', 571510: 'Boyd, Matthew', 601713: 'Pivetta, Nick', 518585: 'Cruz, Fernando', 670059: 'Holderman, Colin', 663559: 'Falter, Bailey', 656945: 'Scott, Tanner', 808963: 'Sasaki, Roki', 605400: 'Nola, Aaron', 676130: 'Buttó, José', 656288: 'Canning, Griffin', 681857: 'Olson, Reese', 663436: 'Martin, Davis', 695243: 'Miller, Mason', 687134: 'Blalock, Bradley', 676664: 'Sears, JP', 608566: 'Márquez, Germán', 663738: 'Lynch IV, Daniel', 608372: 'Sugano, Tomoyuki', 660896: 'Alcala, Jorge', 681293: 'Arrighetti, Spencer', 605130: 'Barlow, Scott', 660730: 'Rodriguez, Elvin', 663903: 'Singer, Brady', 592426: 'Jackson, Luke', 594798: 'deGrom, Jacob', 671737: 'Bradley, Taj', 666120: 'Anderson, Ian', 663474: 'McKenzie, Triston', 676440: 'Bibee, Tanner', 686799: 'Kochanowicz, Jack', 682243: 'Miller, Bryce', 592662: 'Ray, Robbie', 607481: 'Bummer, Aaron', 691594: 'Sanoja, Javier', 677161: 'Kelly, Zack', 621381: 'Strahm, Matt', 808967: 'Yamamoto, Yoshinobu', 676684: 'Vest, Will', 669060: 'Wilson, Bryse', 685126: 'Eisert, Brandon', 679885: 'Martinez, Justin', 621363: 'Poche, Colin', 678020: 'Halvorsen, Seth', 622554: 'Domínguez, Seranthony', 542888: 'Armstrong, Shawn', 656730: 'Megill, Trevor', 702674: 'Dana, Caden', 683769: 'Gaddis, Hunter', 666171: 'Zeferjahn, Ryan', 667755: 'Soriano, José', 641329: 'Baker, Bryan', 656557: 'Houck, Tanner', 641482: 'Cortes, Nestor', 672782: 'Gómez, Yoendrys', 676534: 'Faucher, Calvin', 683004: 'Leiter, Jack', 592155: 'Booser, Cam', 678226: 'Hernández, Daysbel', 693821: 'Elder, Bryce', 605483: 'Snell, Blake', 678692: 'Henriquez, Ronny', 645261: 'Alcantara, Sandy', 664875: 'Lawrence, Justin', 686642: 'Ellard, Fraser', 646242: 'Díaz, Jhonathan', 663423: 'Thornton, Trent', 647336: 'Soroka, Michael', 670766: 'McCaughan, Darren', 472610: 'García, Luis', 607192: 'Glasnow, Tyler', 593974: 'Peralta, Wandy', 606996: 'Hart, Kyle', 641343: 'Bauers, Jake', 650644: 'Civale, Aaron', 677976: 'Dobnak, Randy', 593576: 'Neris, Héctor', 671162: 'Thomas, Connor', 687330: 'Kelly, Kevin', 664076: 'Cleavinger, Garrett', 663992: 'Lovelady, Richard', 453286: 'Scherzer, Max', 628317: 'Maeda, Kenta', 681190: 'Vásquez, Randy', 544150: 'Suárez, Albert', 670280: 'Bednar, David', 641745: 'Keller, Brad', 625643: 'López, Reynaldo', 670032: 'Lopez, Nicky', 696270: 'Johnson, Ryan', 543056: 'Coulombe, Danny', 621237: 'Alvarado, José', 678024: 'Vasil, Mike', 669622: 'Bender, Anthony', 663947: 'Holton, Tyler', 656986: 'Sousa, Bennett', 596271: 'Lawrence, Casey', 681982: 'Anderson, Grant', 671131: 'Rutledge, Jackson', 641835: 'Mayza, Tim', 663767: 'Shugart, Chase', 657585: 'Garrett, Reed', 676428: 'Hurter, Brant', 641656: 'Hamilton, Ian', 642232: 'Yarbrough, Ryan', 571578: 'Corbin, Patrick', 642100: 'Speier, Gabe', 664285: 'Valdez, Framber', 476594: 'Stock, Robert', 592454: 'Kahnle, Tommy', 493603: 'Ottavino, Adam', 669422: 'Sauer, Matt', 663969: 'Phillips, Tyler', 642397: 'Soto, Gregory', 666974: 'Cano, Yennier', 657044: 'Thompson, Ryan', 666619: 'Santos, Gregory', 595014: 'Treinen, Blake', 667463: 'King, John', 670970: 'Morejon, Adrian', 663855: 'Hicks, Jordan', 519008: 'McFarland, T.J.', 573009: 'Mantiply, Joe', 657612: 'Hill, Tim', 573186: 'Stroman, Marcus', 661403: 'Clase, Emmanuel', 669724: 'Hanifee, Brenan', 650489: 'Castro, Willi', 640902: 'Pereda, Jhonny', 668716: 'Murdock, Noah', 672841: 'Vargas, Carlos', 676106: 'Hancock, Emerson', 663687: 'Harris, Hogan', 642048: 'Saucedo, Tayler', 622786: 'Tinoco, Jesus', 663893: 'Little, Brendon', 676477: 'Whitlock, Garrett', 518397: 'Alexander, Scott', 682842: 'Uribe, Abner', 656234: 'Bird, Jake', 621366: 'Borucki, Ryan', 607200: 'Fedde, Erick', 643410: 'Leiter Jr., Mark', 621199: 'Bowman, Matt', 657649: 'Koenig, Jared', 665625: 'Peguero, Elvis', 676254: 'Walker, Ryan', 573124: 'Rogers, Taylor', 571927: 'Matz, Steven', 664849: 'Young, Danny', 534910: 'Hahn, Jesse', 623352: 'Hader, Josh', 694738: 'Roupp, Landen', 642585: 'Bautista, Félix', 650911: 'Sánchez, Cristopher', 681867: 'Criswell, Cooper', 657514: 'Bernardino, Brennan', 678606: 'Ferrer, Jose A.', 666808: 'Doval, Camilo', 643511: 'Rogers, Tyler', 623437: 'Topa, Justin', 680729: 'McDaniels, Garrett', 702352: 'Bivens, Spencer', 445926: 'Chavez, Jesse', 664208: 'Maton, Phil', 445276: 'Jansen, Kenley', 669203: 'Burnes, Corbin', 687765: 'Spence, Mitch', 677649: 'Duran, Ezequiel', 668933: 'Ashcraft, Graham', 681799: 'Roberts, Ethan', 689254: 'Fluharty, Mason'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m stats_list = []\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pitcher_id \u001b[38;5;129;01min\u001b[39;00m unique_pitcher_ids:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     stats = \u001b[43mget_fangraphs_pitcher_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpitcher_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2025\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     89\u001b[39m         stats_list.append(stats)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mget_fangraphs_pitcher_stats\u001b[39m\u001b[34m(pitcher_id, season, stats)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_fangraphs_pitcher_stats\u001b[39m(pitcher_id: \u001b[38;5;28mint\u001b[39m, season: \u001b[38;5;28mint\u001b[39m, stats: \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     df_fangraphs = \u001b[43mfangraphs_pitching_leaderboards\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseason\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseason\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df_fangraphs.empty:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo data available for season \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mfangraphs_pitching_leaderboards\u001b[39m\u001b[34m(season)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfangraphs_pitching_leaderboards\u001b[39m(season: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m      5\u001b[39m     url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&season=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&season1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&ind=0&qual=0&type=8&month=0&pageitems=500000\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to fetch data for season \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/urllib3/response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1060\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/urllib3/response.py:1222\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1221\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1223\u001b[39m decoded = \u001b[38;5;28mself\u001b[39m._decode(\n\u001b[32m   1224\u001b[39m     chunk, decode_content=decode_content, flush_decoder=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1225\u001b[39m )\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/github.com/mikerabs/StuffLocation/env/lib/python3.12/site-packages/urllib3/response.py:1159\u001b[39m, in \u001b[36mHTTPResponse._handle_chunk\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt < \u001b[38;5;28mself\u001b[39m.chunk_left:\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = \u001b[38;5;28mself\u001b[39m.chunk_left - amt\n\u001b[32m   1161\u001b[39m     returned_chunk = value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:640\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[32m    634\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[32m    635\u001b[39m \n\u001b[32m    636\u001b[39m \u001b[33;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[33;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m    642\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1253\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1251\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1252\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fangraphs_pitching_leaderboards(season: int):\n",
    "    url = f\"https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&season={season}&season1={season}&ind=0&qual=0&type=8&month=0&pageitems=500000\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for season {season}. Status code: {response.status_code}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data=data['data'])\n",
    "    return df\n",
    "\n",
    "# Step 1: Extract unique Pitcher_ID values\n",
    "unique_pitcher_ids = df_2023_total['Pitcher_ID'].unique()\n",
    "print(\"Unique Pitcher IDs:\", unique_pitcher_ids)\n",
    "\n",
    "# Step 2: Create a mapping of Pitcher_ID to Pitcher name (using the first occurrence)\n",
    "pitcher_mapping = df_2023_total.drop_duplicates(subset=['Pitcher_ID'])[['Pitcher_ID', 'Pitcher']].set_index('Pitcher_ID')['Pitcher'].to_dict()\n",
    "print(\"Pitcher Mapping:\", pitcher_mapping)\n",
    "\n",
    "# Step 3: Function to fetch and format stats for a given Pitcher_ID\n",
    "def get_fangraphs_pitcher_stats(pitcher_id: int, season: int, stats: list):\n",
    "    df_fangraphs = fangraphs_pitching_leaderboards(season=season)\n",
    "    \n",
    "    if df_fangraphs.empty:\n",
    "        print(f\"No data available for season {season}\")\n",
    "        return None\n",
    "    \n",
    "    df_fangraphs_pitcher = df_fangraphs[df_fangraphs['xMLBAMID'] == pitcher_id][stats].reset_index(drop=True)\n",
    "    \n",
    "    if df_fangraphs_pitcher.empty:\n",
    "        print(f\"No stats found for pitcher with ID {pitcher_id} in {season}\")\n",
    "        return None\n",
    "    \n",
    "    # Format the values using fangraphs_stats_dict\n",
    "    formatted_stats = {}\n",
    "    for stat in stats:\n",
    "        value = df_fangraphs_pitcher[stat].iloc[0]\n",
    "        formatted_stats[stat] = format(value, fangraphs_stats_dict.get(stat, {'format': '.2f'})['format']) if value != '---' else '---'\n",
    "    \n",
    "    return {\n",
    "        'Pitcher_ID': pitcher_id,\n",
    "        'Pitcher': pitcher_mapping.get(pitcher_id, 'Unknown'),\n",
    "        **formatted_stats\n",
    "    }\n",
    "\n",
    "# FANGRAPHS STATS DICT (same as provided)\n",
    "fangraphs_stats_dict = {\n",
    "    'IP': {'table_header': '$\\\\bf{IP}$', 'format': '.1f'},\n",
    "    'TBF': {'table_header': '$\\\\bf{PA}$', 'format': '.0f'},\n",
    "    'AVG': {'table_header': '$\\\\bf{AVG}$', 'format': '.3f'},\n",
    "    'K/9': {'table_header': '$\\\\bf{K/9}$', 'format': '.2f'},\n",
    "    'BB/9': {'table_header': '$\\\\bf{BB/9}$', 'format': '.2f'},\n",
    "    'K/BB': {'table_header': '$\\\\bf{K/BB}$', 'format': '.2f'},\n",
    "    'HR/9': {'table_header': '$\\\\bf{HR/9}$', 'format': '.2f'},\n",
    "    'K%': {'table_header': '$\\\\bf{K\\\\%}$', 'format': '.1%'},\n",
    "    'BB%': {'table_header': '$\\\\bf{BB\\\\%}$', 'format': '.1%'},\n",
    "    'K-BB%': {'table_header': '$\\\\bf{K-BB\\\\%}$', 'format': '.1%'},\n",
    "    'WHIP': {'table_header': '$\\\\bf{WHIP}$', 'format': '.2f'},\n",
    "    'BABIP': {'table_header': '$\\\\bf{BABIP}$', 'format': '.3f'},\n",
    "    'LOB%': {'table_header': '$\\\\bf{LOB\\\\%}$', 'format': '.1%'},\n",
    "    'xFIP': {'table_header': '$\\\\bf{xFIP}$', 'format': '.2f'},\n",
    "    'FIP': {'table_header': '$\\\\bf{FIP}$', 'format': '.2f'},\n",
    "    'H': {'table_header': '$\\\\bf{H}$', 'format': '.0f'},\n",
    "    '2B': {'table_header': '$\\\\bf{2B}$', 'format': '.0f'},\n",
    "    '3B': {'table_header': '$\\\\bf{3B}$', 'format': '.0f'},\n",
    "    'R': {'table_header': '$\\\\bf{R}$', 'format': '.0f'},\n",
    "    'ER': {'table_header': '$\\\\bf{ER}$', 'format': '.0f'},\n",
    "    'HR': {'table_header': '$\\\\bf{HR}$', 'format': '.0f'},\n",
    "    'BB': {'table_header': '$\\\\bf{BB}$', 'format': '.0f'},\n",
    "    'IBB': {'table_header': '$\\\\bf{IBB}$', 'format': '.0f'},\n",
    "    'HBP': {'table_header': '$\\\\bf{HBP}$', 'format': '.0f'},\n",
    "    'SO': {'table_header': '$\\\\bf{SO}$', 'format': '.0f'},\n",
    "    'OBP': {'table_header': '$\\\\bf{OBP}$', 'format': '.0f'},\n",
    "    'SLG': {'table_header': '$\\\\bf{SLG}$', 'format': '.0f'},\n",
    "    'ERA': {'table_header': '$\\\\bf{ERA}$', 'format': '.2f'},\n",
    "    'wOBA': {'table_header': '$\\\\bf{wOBA}$', 'format': '.3f'},\n",
    "    'G': {'table_header': '$\\\\bf{G}$', 'format': '.0f'}\n",
    "}\n",
    "\n",
    "# Step 4: Define stats to fetch and fetch stats for each unique Pitcher_ID\n",
    "selected_stats = ['IP', 'TBF', 'WHIP', 'ERA', 'FIP', 'K%', 'BB%', 'K-BB%']\n",
    "stats_list = []\n",
    "\n",
    "for pitcher_id in unique_pitcher_ids:\n",
    "    stats = get_fangraphs_pitcher_stats(pitcher_id, season=2025, stats=selected_stats)\n",
    "    if stats is not None:\n",
    "        stats_list.append(stats)\n",
    "\n",
    "# Step 5: Create a DataFrame from the collected stats\n",
    "if stats_list:\n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    print(\"\\n2025 Pitcher Stats DataFrame:\")\n",
    "    print(df_stats)\n",
    "else:\n",
    "    print(\"No valid stats were retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Fangraphs leaderboard once for the season...\n",
      "Total unique pitcher IDs: 433\n",
      "Processing 1/433: Pitcher ID 605452\n",
      "Processing 2/433: Pitcher ID 621383\n",
      "Processing 3/433: Pitcher ID 660853\n",
      "Processing 4/433: Pitcher ID 672578\n",
      "Processing 5/433: Pitcher ID 666200\n",
      "Processing 6/433: Pitcher ID 680885\n",
      "Processing 7/433: Pitcher ID 685801\n",
      "Processing 8/433: Pitcher ID 669438\n",
      "Processing 9/433: Pitcher ID 641793\n",
      "Processing 10/433: Pitcher ID 676282\n",
      "Processing 11/433: Pitcher ID 605182\n",
      "Processing 12/433: Pitcher ID 596001\n",
      "Processing 13/433: Pitcher ID 686563\n",
      "Processing 14/433: Pitcher ID 668909\n",
      "Processing 15/433: Pitcher ID 680704\n",
      "Processing 16/433: Pitcher ID 547973\n",
      "Processing 17/433: Pitcher ID 554340\n",
      "Processing 18/433: Pitcher ID 643338\n",
      "Processing 19/433: Pitcher ID 458677\n",
      "Processing 20/433: Pitcher ID 621111\n",
      "Processing 21/433: Pitcher ID 605135\n",
      "Processing 22/433: Pitcher ID 608032\n",
      "Processing 23/433: Pitcher ID 686973\n",
      "Processing 24/433: Pitcher ID 668674\n",
      "Processing 25/433: Pitcher ID 663485\n",
      "Processing 26/433: Pitcher ID 641927\n",
      "Processing 27/433: Pitcher ID 608379\n",
      "Processing 28/433: Pitcher ID 606303\n",
      "Processing 29/433: Pitcher ID 663542\n",
      "Processing 30/433: Pitcher ID 663372\n",
      "Processing 31/433: Pitcher ID 682990\n",
      "Processing 32/433: Pitcher ID 664126\n",
      "Processing 33/433: Pitcher ID 682254\n",
      "Processing 34/433: Pitcher ID 672282\n",
      "Processing 35/433: Pitcher ID 579328\n",
      "Processing 36/433: Pitcher ID 686752\n",
      "Processing 37/433: Pitcher ID 669276\n",
      "Processing 38/433: Pitcher ID 572955\n",
      "Processing 39/433: Pitcher ID 614179\n",
      "Processing 40/433: Pitcher ID 592836\n",
      "Processing 41/433: Pitcher ID 656550\n",
      "Processing 42/433: Pitcher ID 681911\n",
      "Processing 43/433: Pitcher ID 605347\n",
      "Processing 44/433: Pitcher ID 674285\n",
      "Processing 45/433: Pitcher ID 489446\n",
      "Processing 46/433: Pitcher ID 607455\n",
      "Processing 47/433: Pitcher ID 663623\n",
      "Processing 48/433: Pitcher ID 676263\n",
      "Processing 49/433: Pitcher ID 689017\n",
      "Processing 50/433: Pitcher ID 682120\n",
      "Processing 51/433: Pitcher ID 682171\n",
      "Processing 52/433: Pitcher ID 671106\n",
      "Processing 53/433: Pitcher ID 680732\n",
      "Processing 54/433: Pitcher ID 669199\n",
      "Processing 55/433: Pitcher ID 621242\n",
      "Processing 56/433: Pitcher ID 623211\n",
      "Processing 57/433: Pitcher ID 676974\n",
      "Processing 58/433: Pitcher ID 592773\n",
      "Processing 59/433: Pitcher ID 668820\n",
      "Processing 60/433: Pitcher ID 656731\n",
      "Processing 61/433: Pitcher ID 664854\n",
      "Processing 62/433: Pitcher ID 642701\n",
      "Processing 63/433: Pitcher ID 681517\n",
      "Processing 64/433: Pitcher ID 657571\n",
      "Processing 65/433: Pitcher ID 656605\n",
      "Processing 66/433: Pitcher ID 661395\n",
      "Processing 67/433: Pitcher ID 643377\n",
      "Processing 68/433: Pitcher ID 608717\n",
      "Processing 69/433: Pitcher ID 657746\n",
      "Processing 70/433: Pitcher ID 669674\n",
      "Processing 71/433: Pitcher ID 607625\n",
      "Processing 72/433: Pitcher ID 642207\n",
      "Processing 73/433: Pitcher ID 605154\n",
      "Processing 74/433: Pitcher ID 596133\n",
      "Processing 75/433: Pitcher ID 656427\n",
      "Processing 76/433: Pitcher ID 608331\n",
      "Processing 77/433: Pitcher ID 670174\n",
      "Processing 78/433: Pitcher ID 669711\n",
      "Processing 79/433: Pitcher ID 656546\n",
      "Processing 80/433: Pitcher ID 592332\n",
      "Processing 81/433: Pitcher ID 676395\n",
      "Processing 82/433: Pitcher ID 669212\n",
      "Processing 83/433: Pitcher ID 663878\n",
      "Processing 84/433: Pitcher ID 641816\n",
      "Processing 85/433: Pitcher ID 684007\n",
      "Processing 86/433: Pitcher ID 650556\n",
      "Processing 87/433: Pitcher ID 595345\n",
      "Processing 88/433: Pitcher ID 664351\n",
      "Processing 89/433: Pitcher ID 686613\n",
      "Processing 90/433: Pitcher ID 676092\n",
      "Processing 91/433: Pitcher ID 660825\n",
      "Processing 92/433: Pitcher ID 622379\n",
      "Processing 93/433: Pitcher ID 663158\n",
      "Processing 94/433: Pitcher ID 669093\n",
      "Processing 95/433: Pitcher ID 622780\n",
      "Processing 96/433: Pitcher ID 674370\n",
      "Processing 97/433: Pitcher ID 686826\n",
      "Processing 98/433: Pitcher ID 681882\n",
      "Processing 99/433: Pitcher ID 656222\n",
      "Processing 100/433: Pitcher ID 672335\n",
      "Processing 101/433: Pitcher ID 694297\n",
      "Processing 102/433: Pitcher ID 665152\n",
      "Processing 103/433: Pitcher ID 680767\n",
      "Processing 104/433: Pitcher ID 641755\n",
      "Processing 105/433: Pitcher ID 683409\n",
      "Processing 106/433: Pitcher ID 683232\n",
      "Processing 107/433: Pitcher ID 678821\n",
      "Processing 108/433: Pitcher ID 622608\n",
      "Processing 109/433: Pitcher ID 641302\n",
      "Processing 110/433: Pitcher ID 641941\n",
      "Processing 111/433: Pitcher ID 669062\n",
      "Processing 112/433: Pitcher ID 664139\n",
      "Processing 113/433: Pitcher ID 663574\n",
      "Processing 114/433: Pitcher ID 678495\n",
      "Processing 115/433: Pitcher ID 607259\n",
      "Processing 116/433: Pitcher ID 434378\n",
      "Processing 117/433: Pitcher ID 656271\n",
      "Processing 118/433: Pitcher ID 670955\n",
      "Processing 119/433: Pitcher ID 690829\n",
      "Processing 120/433: Pitcher ID 669358\n",
      "Processing 121/433: Pitcher ID 543294\n",
      "Processing 122/433: Pitcher ID 676508\n",
      "Processing 123/433: Pitcher ID 687377\n",
      "Processing 124/433: Pitcher ID 680736\n",
      "Processing 125/433: Pitcher ID 695418\n",
      "Processing 126/433: Pitcher ID 668941\n",
      "Processing 127/433: Pitcher ID 694973\n",
      "Processing 128/433: Pitcher ID 543243\n",
      "Processing 129/433: Pitcher ID 678215\n",
      "Processing 130/433: Pitcher ID 675540\n",
      "Processing 131/433: Pitcher ID 687362\n",
      "Processing 132/433: Pitcher ID 605280\n",
      "Processing 133/433: Pitcher ID 628452\n",
      "Processing 134/433: Pitcher ID 689147\n",
      "Processing 135/433: Pitcher ID 554430\n",
      "Processing 136/433: Pitcher ID 519242\n",
      "Processing 137/433: Pitcher ID 606930\n",
      "Processing 138/433: Pitcher ID 684320\n",
      "Processing 139/433: Pitcher ID 676979\n",
      "Processing 140/433: Pitcher ID 687922\n",
      "Processing 141/433: Pitcher ID 669373\n",
      "Processing 142/433: Pitcher ID 471911\n",
      "Processing 143/433: Pitcher ID 656457\n",
      "Processing 144/433: Pitcher ID 623149\n",
      "Processing 145/433: Pitcher ID 681343\n",
      "Processing 146/433: Pitcher ID 671922\n",
      "Processing 147/433: Pitcher ID 594902\n",
      "Processing 148/433: Pitcher ID 519151\n",
      "Processing 149/433: Pitcher ID 455119\n",
      "Processing 150/433: Pitcher ID 687863\n",
      "Processing 151/433: Pitcher ID 657240\n",
      "Processing 152/433: Pitcher ID 687847\n",
      "Processing 153/433: Pitcher ID 592791\n",
      "Processing 154/433: Pitcher ID 657097\n",
      "Processing 155/433: Pitcher ID 670167\n",
      "Processing 156/433: Pitcher ID 672582\n",
      "Processing 157/433: Pitcher ID 666142\n",
      "Processing 158/433: Pitcher ID 641154\n",
      "Processing 159/433: Pitcher ID 687473\n",
      "Processing 160/433: Pitcher ID 662253\n",
      "Processing 161/433: Pitcher ID 687911\n",
      "Processing 162/433: Pitcher ID 622491\n",
      "Processing 163/433: Pitcher ID 806185\n",
      "Processing 164/433: Pitcher ID 666157\n",
      "Processing 165/433: Pitcher ID 640462\n",
      "Processing 166/433: Pitcher ID 669211\n",
      "Processing 167/433: Pitcher ID 571946\n",
      "Processing 168/433: Pitcher ID 518876\n",
      "Processing 169/433: Pitcher ID 450203\n",
      "Processing 170/433: Pitcher ID 621053\n",
      "Processing 171/433: Pitcher ID 686993\n",
      "Processing 172/433: Pitcher ID 673513\n",
      "Processing 173/433: Pitcher ID 600917\n",
      "Processing 174/433: Pitcher ID 689690\n",
      "Processing 175/433: Pitcher ID 605488\n",
      "Processing 176/433: Pitcher ID 656302\n",
      "Processing 177/433: Pitcher ID 623474\n",
      "Processing 178/433: Pitcher ID 642547\n",
      "Processing 179/433: Pitcher ID 607536\n",
      "Processing 180/433: Pitcher ID 621244\n",
      "Processing 181/433: Pitcher ID 690916\n",
      "Processing 182/433: Pitcher ID 689225\n",
      "Processing 183/433: Pitcher ID 607074\n",
      "Processing 184/433: Pitcher ID 663554\n",
      "Processing 185/433: Pitcher ID 640448\n",
      "Processing 186/433: Pitcher ID 608371\n",
      "Processing 187/433: Pitcher ID 669022\n",
      "Processing 188/433: Pitcher ID 669160\n",
      "Processing 189/433: Pitcher ID 666277\n",
      "Processing 190/433: Pitcher ID 685107\n",
      "Processing 191/433: Pitcher ID 673540\n",
      "Processing 192/433: Pitcher ID 678368\n",
      "Processing 193/433: Pitcher ID 802419\n",
      "Processing 194/433: Pitcher ID 688297\n",
      "Processing 195/433: Pitcher ID 669461\n",
      "Processing 196/433: Pitcher ID 669387\n",
      "Processing 197/433: Pitcher ID 592094\n",
      "Processing 198/433: Pitcher ID 650633\n",
      "Processing 199/433: Pitcher ID 622663\n",
      "Processing 200/433: Pitcher ID 669194\n",
      "Processing 201/433: Pitcher ID 621107\n",
      "Processing 202/433: Pitcher ID 668678\n",
      "Processing 203/433: Pitcher ID 656240\n",
      "Processing 204/433: Pitcher ID 640451\n",
      "Processing 205/433: Pitcher ID 680573\n",
      "Processing 206/433: Pitcher ID 547179\n",
      "Processing 207/433: Pitcher ID 607067\n",
      "Processing 208/433: Pitcher ID 657006\n",
      "Processing 209/433: Pitcher ID 642520\n",
      "Processing 210/433: Pitcher ID 543135\n",
      "Processing 211/433: Pitcher ID 668881\n",
      "Processing 212/433: Pitcher ID 642152\n",
      "Processing 213/433: Pitcher ID 657277\n",
      "Processing 214/433: Pitcher ID 605463\n",
      "Processing 215/433: Pitcher ID 669713\n",
      "Processing 216/433: Pitcher ID 669302\n",
      "Processing 217/433: Pitcher ID 681676\n",
      "Processing 218/433: Pitcher ID 686580\n",
      "Processing 219/433: Pitcher ID 669467\n",
      "Processing 220/433: Pitcher ID 656794\n",
      "Processing 221/433: Pitcher ID 621345\n",
      "Processing 222/433: Pitcher ID 670102\n",
      "Processing 223/433: Pitcher ID 656849\n",
      "Processing 224/433: Pitcher ID 605447\n",
      "Processing 225/433: Pitcher ID 673929\n",
      "Processing 226/433: Pitcher ID 527048\n",
      "Processing 227/433: Pitcher ID 695549\n",
      "Processing 228/433: Pitcher ID 700669\n",
      "Processing 229/433: Pitcher ID 690928\n",
      "Processing 230/433: Pitcher ID 571945\n",
      "Processing 231/433: Pitcher ID 676962\n",
      "Processing 232/433: Pitcher ID 670810\n",
      "Processing 233/433: Pitcher ID 592866\n",
      "Processing 234/433: Pitcher ID 666214\n",
      "Processing 235/433: Pitcher ID 687396\n",
      "Processing 236/433: Pitcher ID 571760\n",
      "Processing 237/433: Pitcher ID 701542\n",
      "Processing 238/433: Pitcher ID 693433\n",
      "Processing 239/433: Pitcher ID 682847\n",
      "Processing 240/433: Pitcher ID 542881\n",
      "Processing 241/433: Pitcher ID 801403\n",
      "Processing 242/433: Pitcher ID 683155\n",
      "Processing 243/433: Pitcher ID 663460\n",
      "Processing 244/433: Pitcher ID 700249\n",
      "Processing 245/433: Pitcher ID 608718\n",
      "Processing 246/433: Pitcher ID 694477\n",
      "Processing 247/433: Pitcher ID 594580\n",
      "Processing 248/433: Pitcher ID 686730\n",
      "Processing 249/433: Pitcher ID 655889\n",
      "Processing 250/433: Pitcher ID 571948\n",
      "Processing 251/433: Pitcher ID 656876\n",
      "Processing 252/433: Pitcher ID 677958\n",
      "Processing 253/433: Pitcher ID 606160\n",
      "Processing 254/433: Pitcher ID 663978\n",
      "Processing 255/433: Pitcher ID 669854\n",
      "Processing 256/433: Pitcher ID 680730\n",
      "Processing 257/433: Pitcher ID 593958\n",
      "Processing 258/433: Pitcher ID 660761\n",
      "Processing 259/433: Pitcher ID 700363\n",
      "Processing 260/433: Pitcher ID 615698\n",
      "Processing 261/433: Pitcher ID 573204\n",
      "Processing 262/433: Pitcher ID 678316\n",
      "Processing 263/433: Pitcher ID 571510\n",
      "Processing 264/433: Pitcher ID 601713\n",
      "Processing 265/433: Pitcher ID 518585\n",
      "Processing 266/433: Pitcher ID 670059\n",
      "Processing 267/433: Pitcher ID 663559\n",
      "Processing 268/433: Pitcher ID 656945\n",
      "Processing 269/433: Pitcher ID 808963\n",
      "Processing 270/433: Pitcher ID 605400\n",
      "Processing 271/433: Pitcher ID 676130\n",
      "Processing 272/433: Pitcher ID 656288\n",
      "Processing 273/433: Pitcher ID 681857\n",
      "Processing 274/433: Pitcher ID 663436\n",
      "Processing 275/433: Pitcher ID 695243\n",
      "Processing 276/433: Pitcher ID 687134\n",
      "Processing 277/433: Pitcher ID 676664\n",
      "Processing 278/433: Pitcher ID 608566\n",
      "Processing 279/433: Pitcher ID 663738\n",
      "Processing 280/433: Pitcher ID 608372\n",
      "Processing 281/433: Pitcher ID 660896\n",
      "Processing 282/433: Pitcher ID 681293\n",
      "Processing 283/433: Pitcher ID 605130\n",
      "Processing 284/433: Pitcher ID 660730\n",
      "Processing 285/433: Pitcher ID 663903\n",
      "Processing 286/433: Pitcher ID 592426\n",
      "Processing 287/433: Pitcher ID 594798\n",
      "Processing 288/433: Pitcher ID 671737\n",
      "Processing 289/433: Pitcher ID 666120\n",
      "Processing 290/433: Pitcher ID 663474\n",
      "Processing 291/433: Pitcher ID 676440\n",
      "Processing 292/433: Pitcher ID 686799\n",
      "Processing 293/433: Pitcher ID 682243\n",
      "Processing 294/433: Pitcher ID 592662\n",
      "Processing 295/433: Pitcher ID 607481\n",
      "Processing 296/433: Pitcher ID 691594\n",
      "Processing 297/433: Pitcher ID 677161\n",
      "Processing 298/433: Pitcher ID 621381\n",
      "Processing 299/433: Pitcher ID 808967\n",
      "Processing 300/433: Pitcher ID 676684\n",
      "Processing 301/433: Pitcher ID 669060\n",
      "Processing 302/433: Pitcher ID 685126\n",
      "Processing 303/433: Pitcher ID 679885\n",
      "Processing 304/433: Pitcher ID 621363\n",
      "Processing 305/433: Pitcher ID 678020\n",
      "Processing 306/433: Pitcher ID 622554\n",
      "Processing 307/433: Pitcher ID 542888\n",
      "Processing 308/433: Pitcher ID 656730\n",
      "Processing 309/433: Pitcher ID 702674\n",
      "Processing 310/433: Pitcher ID 683769\n",
      "Processing 311/433: Pitcher ID 666171\n",
      "Processing 312/433: Pitcher ID 667755\n",
      "Processing 313/433: Pitcher ID 641329\n",
      "Processing 314/433: Pitcher ID 656557\n",
      "Processing 315/433: Pitcher ID 641482\n",
      "Processing 316/433: Pitcher ID 672782\n",
      "Processing 317/433: Pitcher ID 676534\n",
      "Processing 318/433: Pitcher ID 683004\n",
      "Processing 319/433: Pitcher ID 592155\n",
      "Processing 320/433: Pitcher ID 678226\n",
      "Processing 321/433: Pitcher ID 693821\n",
      "Processing 322/433: Pitcher ID 605483\n",
      "Processing 323/433: Pitcher ID 678692\n",
      "Processing 324/433: Pitcher ID 645261\n",
      "Processing 325/433: Pitcher ID 664875\n",
      "Processing 326/433: Pitcher ID 686642\n",
      "Processing 327/433: Pitcher ID 646242\n",
      "Processing 328/433: Pitcher ID 663423\n",
      "Processing 329/433: Pitcher ID 647336\n",
      "Processing 330/433: Pitcher ID 670766\n",
      "Processing 331/433: Pitcher ID 472610\n",
      "Processing 332/433: Pitcher ID 607192\n",
      "Processing 333/433: Pitcher ID 593974\n",
      "Processing 334/433: Pitcher ID 606996\n",
      "Processing 335/433: Pitcher ID 641343\n",
      "Processing 336/433: Pitcher ID 650644\n",
      "Processing 337/433: Pitcher ID 677976\n",
      "Processing 338/433: Pitcher ID 593576\n",
      "Processing 339/433: Pitcher ID 671162\n",
      "Processing 340/433: Pitcher ID 687330\n",
      "Processing 341/433: Pitcher ID 664076\n",
      "Processing 342/433: Pitcher ID 663992\n",
      "Processing 343/433: Pitcher ID 453286\n",
      "Processing 344/433: Pitcher ID 628317\n",
      "Processing 345/433: Pitcher ID 681190\n",
      "Processing 346/433: Pitcher ID 544150\n",
      "Processing 347/433: Pitcher ID 670280\n",
      "Processing 348/433: Pitcher ID 641745\n",
      "Processing 349/433: Pitcher ID 625643\n",
      "Processing 350/433: Pitcher ID 670032\n",
      "Processing 351/433: Pitcher ID 696270\n",
      "Processing 352/433: Pitcher ID 543056\n",
      "Processing 353/433: Pitcher ID 621237\n",
      "Processing 354/433: Pitcher ID 678024\n",
      "Processing 355/433: Pitcher ID 669622\n",
      "Processing 356/433: Pitcher ID 663947\n",
      "Processing 357/433: Pitcher ID 656986\n",
      "Processing 358/433: Pitcher ID 596271\n",
      "Processing 359/433: Pitcher ID 681982\n",
      "Processing 360/433: Pitcher ID 671131\n",
      "Processing 361/433: Pitcher ID 641835\n",
      "Processing 362/433: Pitcher ID 663767\n",
      "Processing 363/433: Pitcher ID 657585\n",
      "Processing 364/433: Pitcher ID 676428\n",
      "Processing 365/433: Pitcher ID 641656\n",
      "Processing 366/433: Pitcher ID 642232\n",
      "Processing 367/433: Pitcher ID 571578\n",
      "Processing 368/433: Pitcher ID 642100\n",
      "Processing 369/433: Pitcher ID 664285\n",
      "Processing 370/433: Pitcher ID 476594\n",
      "Processing 371/433: Pitcher ID 592454\n",
      "Processing 372/433: Pitcher ID 493603\n",
      "Processing 373/433: Pitcher ID 669422\n",
      "Processing 374/433: Pitcher ID 663969\n",
      "Processing 375/433: Pitcher ID 642397\n",
      "Processing 376/433: Pitcher ID 666974\n",
      "Processing 377/433: Pitcher ID 657044\n",
      "Processing 378/433: Pitcher ID 666619\n",
      "Processing 379/433: Pitcher ID 595014\n",
      "Processing 380/433: Pitcher ID 667463\n",
      "Processing 381/433: Pitcher ID 670970\n",
      "Processing 382/433: Pitcher ID 663855\n",
      "Processing 383/433: Pitcher ID 519008\n",
      "Processing 384/433: Pitcher ID 573009\n",
      "Processing 385/433: Pitcher ID 657612\n",
      "Processing 386/433: Pitcher ID 573186\n",
      "Processing 387/433: Pitcher ID 661403\n",
      "Processing 388/433: Pitcher ID 669724\n",
      "Processing 389/433: Pitcher ID 650489\n",
      "Processing 390/433: Pitcher ID 640902\n",
      "Processing 391/433: Pitcher ID 668716\n",
      "Processing 392/433: Pitcher ID 672841\n",
      "Processing 393/433: Pitcher ID 676106\n",
      "Processing 394/433: Pitcher ID 663687\n",
      "Processing 395/433: Pitcher ID 642048\n",
      "Processing 396/433: Pitcher ID 622786\n",
      "Processing 397/433: Pitcher ID 663893\n",
      "Processing 398/433: Pitcher ID 676477\n",
      "Processing 399/433: Pitcher ID 518397\n",
      "Processing 400/433: Pitcher ID 682842\n",
      "Processing 401/433: Pitcher ID 656234\n",
      "Processing 402/433: Pitcher ID 621366\n",
      "Processing 403/433: Pitcher ID 607200\n",
      "Processing 404/433: Pitcher ID 643410\n",
      "Processing 405/433: Pitcher ID 621199\n",
      "Processing 406/433: Pitcher ID 657649\n",
      "Processing 407/433: Pitcher ID 665625\n",
      "Processing 408/433: Pitcher ID 676254\n",
      "Processing 409/433: Pitcher ID 573124\n",
      "Processing 410/433: Pitcher ID 571927\n",
      "Processing 411/433: Pitcher ID 664849\n",
      "Processing 412/433: Pitcher ID 534910\n",
      "Processing 413/433: Pitcher ID 623352\n",
      "Processing 414/433: Pitcher ID 694738\n",
      "Processing 415/433: Pitcher ID 642585\n",
      "Processing 416/433: Pitcher ID 650911\n",
      "Processing 417/433: Pitcher ID 681867\n",
      "Processing 418/433: Pitcher ID 657514\n",
      "Processing 419/433: Pitcher ID 678606\n",
      "Processing 420/433: Pitcher ID 666808\n",
      "Processing 421/433: Pitcher ID 643511\n",
      "Processing 422/433: Pitcher ID 623437\n",
      "Processing 423/433: Pitcher ID 680729\n",
      "Processing 424/433: Pitcher ID 702352\n",
      "Processing 425/433: Pitcher ID 445926\n",
      "Processing 426/433: Pitcher ID 664208\n",
      "Processing 427/433: Pitcher ID 445276\n",
      "Processing 428/433: Pitcher ID 669203\n",
      "Processing 429/433: Pitcher ID 687765\n",
      "Processing 430/433: Pitcher ID 677649\n",
      "Processing 431/433: Pitcher ID 668933\n",
      "Processing 432/433: Pitcher ID 681799\n",
      "Processing 433/433: Pitcher ID 689254\n",
      "\n",
      "2025 NN Pitcher Stats DataFrame:\n",
      "   Pitcher_ID               Pitcher    IP TBF  WHIP   ERA   FIP     K%    BB%  \\\n",
      "0      605452             Ross, Joe   5.2  24  1.24  6.35  7.04  12.5%   4.2%   \n",
      "1      621383         Banks, Tanner   6.0  24  1.33  1.50  2.98  12.5%   8.3%   \n",
      "2      660853  De Los Santos, Enyel   6.1  26  1.11  1.42  0.77  38.5%   3.8%   \n",
      "3      672578     Hernández, Carlos   3.0  17  2.67  6.00  7.65  23.5%  17.6%   \n",
      "4      666200        Luzardo, Jesús  18.0  69  1.00  1.50  1.76  36.2%   7.2%   \n",
      "\n",
      "   K-BB%  \n",
      "0   8.3%  \n",
      "1   4.2%  \n",
      "2  34.6%  \n",
      "3   5.9%  \n",
      "4  29.0%  \n"
     ]
    }
   ],
   "source": [
    "# === Fangraphs stats matching for NN model ===\n",
    "def fangraphs_pitching_leaderboards(season: int):\n",
    "    url = f\"https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&season={season}&season1={season}&ind=0&qual=0&type=8&month=0&pageitems=500000\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for season {season}. Status code: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data=data['data'])\n",
    "\n",
    "unique_pitcher_ids = df_2023_total['Pitcher_ID'].unique()\n",
    "pitcher_mapping = df_2023_total.drop_duplicates(subset=['Pitcher_ID'])[['Pitcher_ID', 'Pitcher']].set_index('Pitcher_ID')['Pitcher'].to_dict()\n",
    "\n",
    "fangraphs_stats_dict = {\n",
    "    'IP': {'table_header': '$\\\\bf{IP}$', 'format': '.1f'},\n",
    "    'TBF': {'table_header': '$\\\\bf{PA}$', 'format': '.0f'},\n",
    "    'AVG': {'table_header': '$\\\\bf{AVG}$', 'format': '.3f'},\n",
    "    'K/9': {'table_header': '$\\\\bf{K/9}$', 'format': '.2f'},\n",
    "    'BB/9': {'table_header': '$\\\\bf{BB/9}$', 'format': '.2f'},\n",
    "    'K/BB': {'table_header': '$\\\\bf{K/BB}$', 'format': '.2f'},\n",
    "    'HR/9': {'table_header': '$\\\\bf{HR/9}$', 'format': '.2f'},\n",
    "    'K%': {'table_header': '$\\\\bf{K\\\\%}$', 'format': '.1%'},\n",
    "    'BB%': {'table_header': '$\\\\bf{BB\\\\%}$', 'format': '.1%'},\n",
    "    'K-BB%': {'table_header': '$\\\\bf{K-BB\\\\%}$', 'format': '.1%'},\n",
    "    'WHIP': {'table_header': '$\\\\bf{WHIP}$', 'format': '.2f'},\n",
    "    'BABIP': {'table_header': '$\\\\bf{BABIP}$', 'format': '.3f'},\n",
    "    'LOB%': {'table_header': '$\\\\bf{LOB\\\\%}$', 'format': '.1%'},\n",
    "    'xFIP': {'table_header': '$\\\\bf{xFIP}$', 'format': '.2f'},\n",
    "    'FIP': {'table_header': '$\\\\bf{FIP}$', 'format': '.2f'},\n",
    "    'ERA': {'table_header': '$\\\\bf{ERA}$', 'format': '.2f'},\n",
    "    'wOBA': {'table_header': '$\\\\bf{wOBA}$', 'format': '.3f'}\n",
    "}\n",
    "\n",
    "def get_fangraphs_pitcher_stats(pitcher_id: int, season: int, stats: list, df_fangraphs: pd.DataFrame):\n",
    "    df_fg = df_fangraphs[df_fangraphs['xMLBAMID'] == pitcher_id][stats].reset_index(drop=True)\n",
    "    if df_fg.empty:\n",
    "        print(f\"No stats found for pitcher ID {pitcher_id}\")\n",
    "        return None\n",
    "    formatted = {stat: format(df_fg[stat].iloc[0], fangraphs_stats_dict.get(stat, {'format': '.2f'})['format'])\n",
    "                 if df_fg[stat].iloc[0] != '---' else '---' for stat in stats}\n",
    "    return {'Pitcher_ID': pitcher_id, 'Pitcher': pitcher_mapping.get(pitcher_id, 'Unknown'), **formatted}\n",
    "\n",
    "selected_stats = ['IP', 'TBF', 'WHIP', 'ERA', 'FIP', 'K%', 'BB%', 'K-BB%']\n",
    "stats_list_nn = []\n",
    "\n",
    "print(\"Fetching Fangraphs leaderboard once for the season...\")\n",
    "df_fangraphs_all = fangraphs_pitching_leaderboards(season=2025)\n",
    "\n",
    "print(f\"Total unique pitcher IDs: {len(unique_pitcher_ids)}\")\n",
    "for idx, pitcher_id in enumerate(unique_pitcher_ids):\n",
    "    print(f\"Processing {idx + 1}/{len(unique_pitcher_ids)}: Pitcher ID {pitcher_id}\")\n",
    "    stats = get_fangraphs_pitcher_stats(pitcher_id, 2025, selected_stats, df_fangraphs_all)\n",
    "    if stats is not None:\n",
    "        stats_list_nn.append(stats)\n",
    "\n",
    "df_stats_nn = pd.DataFrame(stats_list_nn)\n",
    "print(\"\\n2025 NN Pitcher Stats DataFrame:\")\n",
    "print(df_stats_nn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge df_2023_total_pitcher_RV and df_stats on 'Pitcher_ID'\n",
    "df_2023_total_pitcher_RV = df_2023_total_pitcher_RV.merge(df_stats, on='Pitcher_ID', how='left', suffixes=('', '_fangraphs'))\n",
    "df_2023_total_pitcher_RV.sort_values(by='Final_Adjusted_Stuff_Plus', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_total_pitcher_RV_count100 = df_2023_total_pitcher_RV[df_2023_total_pitcher_RV['Count'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PitcherTeam</th>\n",
       "      <th>Final_Adjusted_Stuff_Plus_NN</th>\n",
       "      <th>RunValue</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SF</td>\n",
       "      <td>107.807426</td>\n",
       "      <td>-12.981520</td>\n",
       "      <td>778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MIN</td>\n",
       "      <td>105.127800</td>\n",
       "      <td>-3.796912</td>\n",
       "      <td>484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TB</td>\n",
       "      <td>103.075867</td>\n",
       "      <td>-6.589804</td>\n",
       "      <td>1144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SD</td>\n",
       "      <td>102.958511</td>\n",
       "      <td>-7.792322</td>\n",
       "      <td>1352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LAD</td>\n",
       "      <td>102.890106</td>\n",
       "      <td>-8.425921</td>\n",
       "      <td>1078.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOU</td>\n",
       "      <td>102.767670</td>\n",
       "      <td>-16.534919</td>\n",
       "      <td>1839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHC</td>\n",
       "      <td>101.707001</td>\n",
       "      <td>-5.045211</td>\n",
       "      <td>1632.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NYM</td>\n",
       "      <td>101.613670</td>\n",
       "      <td>-10.437331</td>\n",
       "      <td>1568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PHI</td>\n",
       "      <td>101.429604</td>\n",
       "      <td>-10.359843</td>\n",
       "      <td>1524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOS</td>\n",
       "      <td>101.307053</td>\n",
       "      <td>-8.971064</td>\n",
       "      <td>1798.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MIL</td>\n",
       "      <td>101.150200</td>\n",
       "      <td>3.206286</td>\n",
       "      <td>1371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WSH</td>\n",
       "      <td>100.906647</td>\n",
       "      <td>1.714994</td>\n",
       "      <td>1850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CIN</td>\n",
       "      <td>100.760155</td>\n",
       "      <td>-14.333604</td>\n",
       "      <td>1537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SEA</td>\n",
       "      <td>100.571869</td>\n",
       "      <td>-13.946439</td>\n",
       "      <td>1944.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PIT</td>\n",
       "      <td>100.470238</td>\n",
       "      <td>-9.443876</td>\n",
       "      <td>1788.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TEX</td>\n",
       "      <td>99.284843</td>\n",
       "      <td>-5.198934</td>\n",
       "      <td>1344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>99.238922</td>\n",
       "      <td>-0.576438</td>\n",
       "      <td>1141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DET</td>\n",
       "      <td>98.852234</td>\n",
       "      <td>-5.045110</td>\n",
       "      <td>979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NYY</td>\n",
       "      <td>98.801125</td>\n",
       "      <td>-2.494730</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CLE</td>\n",
       "      <td>98.765076</td>\n",
       "      <td>-7.458600</td>\n",
       "      <td>1154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CWS</td>\n",
       "      <td>98.652237</td>\n",
       "      <td>-0.451426</td>\n",
       "      <td>997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL</td>\n",
       "      <td>98.484985</td>\n",
       "      <td>2.914457</td>\n",
       "      <td>643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TOR</td>\n",
       "      <td>98.078705</td>\n",
       "      <td>-10.314284</td>\n",
       "      <td>1772.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>STL</td>\n",
       "      <td>97.925194</td>\n",
       "      <td>-4.872326</td>\n",
       "      <td>1224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MIA</td>\n",
       "      <td>97.304749</td>\n",
       "      <td>-1.931793</td>\n",
       "      <td>2162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATH</td>\n",
       "      <td>96.988708</td>\n",
       "      <td>8.383737</td>\n",
       "      <td>1081.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KC</td>\n",
       "      <td>96.657730</td>\n",
       "      <td>-11.582206</td>\n",
       "      <td>1720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAL</td>\n",
       "      <td>96.385010</td>\n",
       "      <td>0.716238</td>\n",
       "      <td>1256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>COL</td>\n",
       "      <td>96.240334</td>\n",
       "      <td>1.961466</td>\n",
       "      <td>1167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LAA</td>\n",
       "      <td>94.806442</td>\n",
       "      <td>3.015381</td>\n",
       "      <td>790.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PitcherTeam  Final_Adjusted_Stuff_Plus_NN   RunValue   Count\n",
       "24          SF                    107.807426 -12.981520   778.0\n",
       "17         MIN                    105.127800  -3.796912   484.0\n",
       "26          TB                    103.075867  -6.589804  1144.0\n",
       "22          SD                    102.958511  -7.792322  1352.0\n",
       "14         LAD                    102.890106  -8.425921  1078.0\n",
       "11         HOU                    102.767670 -16.534919  1839.0\n",
       "5          CHC                    101.707001  -5.045211  1632.0\n",
       "18         NYM                    101.613670 -10.437331  1568.0\n",
       "20         PHI                    101.429604 -10.359843  1524.0\n",
       "4          BOS                    101.307053  -8.971064  1798.0\n",
       "16         MIL                    101.150200   3.206286  1371.0\n",
       "29         WSH                    100.906647   1.714994  1850.0\n",
       "6          CIN                    100.760155 -14.333604  1537.0\n",
       "23         SEA                    100.571869 -13.946439  1944.0\n",
       "21         PIT                    100.470238  -9.443876  1788.0\n",
       "27         TEX                     99.284843  -5.198934  1344.0\n",
       "2           AZ                     99.238922  -0.576438  1141.0\n",
       "10         DET                     98.852234  -5.045110   979.0\n",
       "19         NYY                     98.801125  -2.494730  1100.0\n",
       "7          CLE                     98.765076  -7.458600  1154.0\n",
       "9          CWS                     98.652237  -0.451426   997.0\n",
       "1          ATL                     98.484985   2.914457   643.0\n",
       "28         TOR                     98.078705 -10.314284  1772.0\n",
       "25         STL                     97.925194  -4.872326  1224.0\n",
       "15         MIA                     97.304749  -1.931793  2162.0\n",
       "0          ATH                     96.988708   8.383737  1081.0\n",
       "12          KC                     96.657730 -11.582206  1720.0\n",
       "3          BAL                     96.385010   0.716238  1256.0\n",
       "8          COL                     96.240334   1.961466  1167.0\n",
       "13         LAA                     94.806442   3.015381   790.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derive NN-based summary DataFrame\n",
    "df_2023_total_pitcher_RV_nn = df_2023_total.groupby(['Pitcher','Pitcher_ID','PitcherTeam']).agg({\n",
    "    'Final_Adjusted_Stuff_Plus_NN': 'mean',\n",
    "    'RunValue': 'sum',\n",
    "    'launch_speed': 'mean',\n",
    "    'bat_speed': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_2023_total_pitcher_RV_nn['Count'] = df_2023_total.groupby(['Pitcher']).size().reset_index(name='Count')['Count']\n",
    "\n",
    "# Merge with Fangraphs for visualization\n",
    "if not df_stats_nn.empty:\n",
    "    df_2023_total_pitcher_RV_nn = df_2023_total_pitcher_RV_nn.merge(df_stats_nn, on='Pitcher_ID', how='left', suffixes=('', '_fangraphs'))\n",
    "    df_2023_total_pitcher_RV_nn.sort_values(by='Final_Adjusted_Stuff_Plus_NN', ascending=False, inplace=True)\n",
    "    df_2023_total_pitcher_RV_count100_nn = df_2023_total_pitcher_RV_nn[df_2023_total_pitcher_RV_nn['Count'] > 100]\n",
    "\n",
    "#df_2023_total_pitcher_RV_count100_nn\n",
    "\n",
    "#group df_2023_total_pitcher_RV_count100_nn by PitcherTeam\n",
    "df_2023_total_pitcher_RV_count100_nn_grouped = df_2023_total_pitcher_RV_count100_nn.groupby('PitcherTeam').agg({\n",
    "    'Final_Adjusted_Stuff_Plus_NN': 'mean',\n",
    "    'RunValue': 'sum',\n",
    "    'Count': 'sum'\n",
    "}).reset_index()\n",
    "#sort by Final_Adjusted_Stuff_Plus_NN\n",
    "df_2023_total_pitcher_RV_count100_nn_grouped.sort_values(by='Final_Adjusted_Stuff_Plus_NN', ascending=False, inplace=True)\n",
    "df_2023_total_pitcher_RV_count100_nn_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               RunValue   R-squared:                       0.051\n",
      "Model:                            OLS   Adj. R-squared:                  0.046\n",
      "Method:                 Least Squares   F-statistic:                     9.809\n",
      "Date:                Thu, 10 Apr 2025   Prob (F-statistic):            0.00202\n",
      "Time:                        10:11:57   Log-Likelihood:                -402.09\n",
      "No. Observations:                 184   AIC:                             808.2\n",
      "Df Residuals:                     182   BIC:                             814.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "const                        12.5972      4.162      3.027      0.003       4.386      20.809\n",
      "Final_Adjusted_Stuff_Plus    -0.1307      0.042     -3.132      0.002      -0.213      -0.048\n",
      "==============================================================================\n",
      "Omnibus:                        2.061   Durbin-Watson:                   1.865\n",
      "Prob(Omnibus):                  0.357   Jarque-Bera (JB):                1.859\n",
      "Skew:                           0.047   Prob(JB):                        0.395\n",
      "Kurtosis:                       3.484   Cond. No.                     2.60e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.6e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#x = Final_Adjusted_Stuff_Plus y = RunValue\n",
    "x = df_2023_total_pitcher_RV_count100['Final_Adjusted_Stuff_Plus']\n",
    "y = df_2023_total_pitcher_RV_count100['RunValue']\n",
    "#ols\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X).fit()\n",
    "# Print the summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               RunValue   R-squared:                       0.046\n",
      "Model:                            OLS   Adj. R-squared:                  0.042\n",
      "Method:                 Least Squares   F-statistic:                     10.69\n",
      "Date:                Sat, 12 Apr 2025   Prob (F-statistic):            0.00125\n",
      "Time:                        10:47:42   Log-Likelihood:                -468.50\n",
      "No. Observations:                 223   AIC:                             941.0\n",
      "Df Residuals:                     221   BIC:                             947.8\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "const                            6.6683      2.258      2.953      0.003       2.218      11.118\n",
      "Final_Adjusted_Stuff_Plus_NN    -0.0737      0.023     -3.270      0.001      -0.118      -0.029\n",
      "==============================================================================\n",
      "Omnibus:                       11.742   Durbin-Watson:                   1.913\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               23.385\n",
      "Skew:                          -0.197   Prob(JB):                     8.36e-06\n",
      "Kurtosis:                       4.537   Cond. No.                     1.70e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.7e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# OLS regression between NN Final_Adjusted_Stuff_Plus and RunValue\n",
    "x_nn = df_2023_total_pitcher_RV_count100_nn['Final_Adjusted_Stuff_Plus_NN']\n",
    "y_nn = df_2023_total_pitcher_RV_count100_nn['RunValue']\n",
    "X_nn = sm.add_constant(x_nn)\n",
    "model_nn = sm.OLS(y_nn, X_nn).fit()\n",
    "print(model_nn.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
